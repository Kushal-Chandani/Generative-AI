{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "name": "Linear_Algebra_Matrix_Operations_and_Deep_Learning_Intro.ipynb",
    "language_info": {
      "name": "python"
    },
    "authors": [
      {
        "name": "Dr. Adnan Masood (with ChatGPT)",
        "email": ""
      }
    ],
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "#                                                               #\n",
        "#  CS435 Generative AI: Security, Ethics and Governance         #\n",
        "#                                                               #\n",
        "#  Instructor: Dr. Adnan Masood                                 #\n",
        "#  Contact:    adnanmasood@gmail.com                            #\n",
        "#                                                               #\n",
        "#  Notebook is MIT Licensed                                     #\n",
        "#################################################################"
      ],
      "metadata": {
        "id": "28bkoJsxOx9C"
      },
      "id": "28bkoJsxOx9C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J_Afo7jOpmw"
      },
      "source": [
        "# **Linear Algebra, Matrix Operations, and the Foundations of Deep Learning**\n",
        "---\n",
        "Welcome, everyone! Today, we are going to explore the magical world of **Linear Algebra**—specifically focusing on **matrix operations**, **matrices multiplication (matmul)**, **eigenvectors**, and how all these connect to **neural networks**, **deep learning**, **Generative AI**, and **Large Language Models (LLMs)**.\n",
        "\n",
        "We'll approach this in **five levels**, from a curious beginner to an advanced researcher. Along the way, we’ll build intuition, see a brief history, do mock calculations, and implement working examples in **PyTorch**. By the end, you’ll understand not just *how*, but *why* these concepts are so important to modern AI.\n",
        "\n",
        "---"
      ],
      "id": "6J_Afo7jOpmw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "eGXC9mHgOqVp"
      },
      "id": "eGXC9mHgOqVp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3wTcafuOpmy"
      },
      "source": [
        "## **1. Building the Intuiton:**"
      ],
      "id": "c3wTcafuOpmy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUgc02hvOpmz"
      },
      "source": [
        "Imagine you have some rows of numbers and columns of numbers—this is a *matrix*. If you have two such matrices, you can *add* them by adding the matching numbers, or *multiply* them in a special way to get another set of rows and columns. It’s kind of like mixing up ingredients in a recipe: you take certain parts of one and certain parts of another, and you get a brand-new result.\n",
        "\n",
        "Eigenvectors (pronounced “eye-gen-vectors”) are special directions inside a matrix that don't get turned around when you apply the matrix—like a magic arrow that points in a direction that never changes no matter what you do. These are super important in math and help us solve puzzles about how data moves, changes, or scales.\n",
        "\n",
        "All this is important in computers because we need to handle pictures, words, and numbers. If you want a computer to learn to recognize cats in pictures (like in a phone app or in YouTube), it uses these matrix ideas to figure out patterns. That's what deep learning is about—using math to teach computers to learn from examples!"
      ],
      "id": "kUgc02hvOpmz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaRqQAvROpmz"
      },
      "source": [
        "**Digging Deeper**\n",
        "\n",
        "Linear algebra is the study of lines, planes, and volumes in many dimensions, but in practical terms, we often represent those things with *matrices* (grids of numbers). Matrix multiplication is more than just multiplying numbers—it’s a set of rules for combining rows of one matrix with columns of another, to produce a new grid of numbers.\n",
        "\n",
        "An *eigenvector* is a vector that, when a matrix acts on it, just gets scaled by some factor (the *eigenvalue*) instead of changing direction. This concept is used in many algorithms, including finding patterns or compressing large data sets.\n",
        "\n",
        "In **deep learning** (the technology behind LLMs, image recognition, etc.), we use these matrix operations to transform data step by step, so the computer can learn complex functions that can classify images or generate new text."
      ],
      "id": "gaRqQAvROpmz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNnHiKr4Opmz"
      },
      "source": [
        "Matrices are at the heart of linear transformations. If $A$ is an $m \\times n$ matrix and $x$ is an $n$-dimensional vector, then the product $Ax$ is an $m$-dimensional vector. This operation is fundamental because it represents linear transformations from an $n$-dimensional space to an $m$-dimensional space.\n",
        "\n",
        "For eigenvectors, formally, if $A$ is a square matrix, an eigenvector $v$ satisfies:\n",
        "\n",
        "$$\n",
        "A v = \\lambda v\n",
        "$$\n",
        "\n",
        "for some scalar $\\lambda$ called the *eigenvalue*. If you think about it, applying the transformation $A$ to $v$ results in the same direction, but possibly a different magnitude.\n",
        "\n",
        "In deep learning, we stack multiple linear transformations (plus non-linear activations). Weight matrices hold the parameters that get learned during training, and we combine them with biases and activation functions. The power of deep networks comes from layering many transformations so we can approximate very complex functions.\n"
      ],
      "id": "oNnHiKr4Opmz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhpDmMtuOpmz"
      },
      "source": [
        "### **Level D: Graduate Explorers**\n",
        "We delve deeper into spectral decompositions, diagonalization, and the concept of covariance matrices. In machine learning, the decomposition of the covariance matrix into eigenvectors (Principal Component Analysis) helps reduce dimensionality. For a real symmetric matrix \\(A\\), the spectral theorem ensures an orthogonal eigenbasis.\n",
        "\n",
        "Eigenvalues also play a critical role in the optimization of neural networks—understanding the Hessian matrix’s eigenvalues can tell us about the curvature of the loss surface. Large eigenvalues might indicate a steep direction, which can lead to instability in gradient-based optimization.\n",
        "\n",
        "In large language models (LLMs), these transformations become extremely high-dimensional. Techniques such as matrix factorization, attention mechanisms, and advanced optimizations rely on heavy usage of linear algebra.\n"
      ],
      "id": "qhpDmMtuOpmz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9R4sR4KOpm0"
      },
      "source": [
        "\n",
        "Research in deep learning architecture design often revolves around advanced linear algebra concepts. For example, analyzing the expressivity of neural networks can involve looking at the rank of weight matrices in each layer, or studying the effect of regularization on the singular values. Generative models like Variational Autoencoders (VAEs) or Diffusion Models rely on transformations that are linear plus carefully selected non-linear components, optimized via high-dimensional gradient-based methods.\n",
        "\n",
        "Eigenvalues and eigenvectors are integral to stability and expressivity analyses, as well as to compressed representations (e.g., matrix/tensor decompositions). The entire field of model compression (pruning, factorization, low-rank approximation) is heavily grounded in linear algebra.\n",
        "\n",
        "---"
      ],
      "id": "i9R4sR4KOpm0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMoaJgQtOpm0"
      },
      "source": [
        "## **2. Intuition Behind the Technology**\n",
        "**Why does linear algebra show up everywhere in deep learning?** Because everything we do with data—whether images, text, or signals—eventually gets converted into numbers (vectors). A neural network is essentially a sequence of matrix multiplications plus some squishing functions (activations). If the matrix multiplications are well-chosen (trained with data), they can do everything from identifying spam emails to writing poetry."
      ],
      "id": "BMoaJgQtOpm0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmvXsOQvOpm0"
      },
      "source": [
        "## **3. Brief History: Invention and Underlying Tech**\n",
        "- **Linear Algebra** has roots going back to the study of systems of linear equations (e.g., solutions to multiple equations with multiple unknowns). Mathematicians like Gauss, Euler, and others formalized these ideas.\n",
        "- **Matrix multiplication** was well-understood in the 19th century, with further expansions in the 20th century—leading to the concept of transformations in higher dimensions.\n",
        "- **Eigenvalues and eigenvectors** were studied intensively by mathematicians like Hilbert and others in the context of linear transformations, diagonalization, and functional analysis.\n",
        "- **Neural Networks** (1940s – 1960s) started with perceptrons and continued with multi-layer perceptrons in the 1980s and 1990s, culminating in “Deep Learning” breakthroughs around 2012 when large-scale matrix operations (on GPUs) and huge datasets became feasible.\n",
        "- **Generative AI & LLMs** rose with the advent of the Transformer architecture (2017), which relies on linear algebra for multi-head attention, matrix factorization, and large-scale optimization.\n",
        "\n",
        "Today, we see that powerful computing on large matrices/tensors is key to advanced AI.\n",
        "\n",
        "---"
      ],
      "id": "LmvXsOQvOpm0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3unnxs3Opm0"
      },
      "source": [
        "## **4. Underlying Tech (How It Works)**\n",
        "Modern AI frameworks like PyTorch or TensorFlow are optimized for *tensor operations*, which are generalizations of matrices to higher dimensions. Under the hood, you’re effectively doing a bunch of matrix multiplications and additions, all accelerated by GPUs.\n",
        "\n",
        "### Key Steps in Using Linear Algebra for Deep Learning:\n",
        "1. **Represent** your data as vectors/matrices/tensors.\n",
        "2. **Multiply** by weight matrices (learnable parameters) and **add** biases.\n",
        "3. **Apply** a non-linear transformation (activation function).\n",
        "4. **Stack** multiple layers (matrix + activation) to build *depth*.\n",
        "5. **Calculate** a loss function.\n",
        "6. **Use** gradient descent or similar algorithms to update weight matrices.\n",
        "\n",
        "---"
      ],
      "id": "X3unnxs3Opm0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUpdy3nQOpm0"
      },
      "source": [
        "## **5. ELI5 Math: Building the Foundations**\n",
        "\n",
        "### Matrix Multiplication\n",
        "If you have an $m \\times n$ matrix $A$ and an $n \\times p$ matrix $B$, the product $C = A B$ is an $m \\times p$ matrix. The entry $C_{ij}$ is computed by taking the $i$-th row of $A$ and the $j$-th column of $B$, multiplying corresponding elements, and summing up:\n",
        "\n",
        "$$\n",
        "C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}\n",
        "$$\n",
        "\n",
        "### Eigenvalues and Eigenvectors\n",
        "For a square matrix $A$ ($n \\times n$), an eigenvector $v \\neq 0$ and an eigenvalue $\\lambda$ satisfy:\n",
        "\n",
        "$$\n",
        "A v = \\lambda v\n",
        "$$\n",
        "\n",
        "This means $v$ remains in the same *direction* after being transformed by $A$, but is scaled by $\\lambda$.\n",
        "\n",
        "### Weights and Biases in a Simple Neural Network\n",
        "- A **weight** matrix $W$ transforms input vectors into new vectors.\n",
        "- A **bias** vector $b$ shifts the output.\n",
        "- Example layer output: $\\text{out} = W x + b$\n",
        "\n",
        "### Connection to Deep Learning\n",
        "Deep neural networks chain these transformations together, e.g.:\n",
        "\n",
        "$$\n",
        "x^{(1)} = f(W^{(1)} x^{(0)} + b^{(1)})\n",
        "$$\n",
        "\n",
        "$$\n",
        "x^{(2)} = f(W^{(2)} x^{(1)} + b^{(2)})\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\dots\n",
        "$$\n",
        "\n",
        "and so on, where $f$ is an activation function (like ReLU).\n"
      ],
      "id": "HUpdy3nQOpm0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iUD7pgPOpm1"
      },
      "source": [
        "## **6. Illustrative Example with PyTorch (ELI5 Style)**"
      ],
      "id": "1iUD7pgPOpm1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYeMf1dLOpm1"
      },
      "source": [
        "Let’s do some basic matrix operations and find eigenvalues/eigenvectors using **PyTorch**. We’ll keep it simple so everyone can see how it works."
      ],
      "id": "rYeMf1dLOpm1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDmXrrxSOpm1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd85b404-f683-4756-c6ac-49e1f16428de"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "# Let's create two matrices A and B\n",
        "A = torch.tensor([[1.0, 2.0],\n",
        "                  [3.0, 4.0]])\n",
        "B = torch.tensor([[2.0, 0.0],\n",
        "                  [1.0, 2.0]])\n",
        "\n",
        "print('Matrix A:\\n', A)\n",
        "print('Matrix B:\\n', B)\n",
        "\n",
        "# Matrix multiplication\n",
        "C = torch.matmul(A, B)\n",
        "print('\\nA x B = C:\\n', C)"
      ],
      "execution_count": null,
      "id": "rDmXrrxSOpm1",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A:\n",
            " tensor([[1., 2.],\n",
            "        [3., 4.]])\n",
            "Matrix B:\n",
            " tensor([[2., 0.],\n",
            "        [1., 2.]])\n",
            "\n",
            "A x B = C:\n",
            " tensor([[ 4.,  4.],\n",
            "        [10.,  8.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uERMuq14Opm2"
      },
      "source": [
        "Next, let's compute the eigenvalues and eigenvectors of a matrix. We'll pick **A** for that example."
      ],
      "id": "uERMuq14Opm2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WV25mi8Opm2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8c433e8-75dc-4d90-fb13-62298fc378f6"
      },
      "source": [
        "# Eigen decomposition of A\n",
        "eigenvalues, eigenvectors = torch.linalg.eig(A)\n",
        "\n",
        "print('Eigenvalues of A:', eigenvalues)\n",
        "print('Eigenvectors of A:\\n', eigenvectors)\n",
        "\n",
        "# Let's verify the first eigenpair by a manual matrix multiplication\n",
        "v = eigenvectors[:,0]  # first eigenvector\n",
        "lambda_v = eigenvalues[0]\n",
        "\n",
        "# A v should be approximately lambda_v * v\n",
        "Av = torch.matmul(A.type(torch.cfloat), v) #TODO- whats wrong here?\n",
        "#HINT- # may be Cast A to complex float to perform the multiplication with a complex vector\n",
        "lambda_v_times_v = lambda_v * v\n",
        "\n",
        "print('\\nCheck A * v vs lambda_v * v')\n",
        "print('A * v =', Av)\n",
        "print('lambda_v * v =', lambda_v_times_v)"
      ],
      "execution_count": null,
      "id": "4WV25mi8Opm2",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvalues of A: tensor([-0.3723+0.j,  5.3723+0.j])\n",
            "Eigenvectors of A:\n",
            " tensor([[-0.8246+0.j, -0.4160+0.j],\n",
            "        [ 0.5658+0.j, -0.9094+0.j]])\n",
            "\n",
            "Check A * v vs lambda_v * v\n",
            "A * v = tensor([ 0.3070+0.j, -0.2106+0.j])\n",
            "lambda_v * v = tensor([ 0.3070-0.j, -0.2106+0.j])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBdDnfUJOpm2"
      },
      "source": [
        "> **Note**: The eigenvalues might be complex if the matrix is not perfectly symmetric or if it’s not diagonalizable in real numbers. In our case, matrix \\(A\\) is real and should yield real eigenvalues (though with floating point computations, you might see small imaginary parts)."
      ],
      "id": "gBdDnfUJOpm2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxaMz35dOpm2"
      },
      "source": [
        "## **7. Example Calculations & Terms**\n",
        "\n",
        "1. **Weight ($W$)**: Think of it as a scaling factor or a matrix that changes the shape of your data.\n",
        "2. **Bias ($b$)**: A shift term added to the multiplication result.\n",
        "3. **Forward Pass**: The process where you compute the output of a network by applying all transformations in sequence.\n",
        "4. **Loss Function**: The measure of how far your predictions are from the truth.\n",
        "5. **Backpropagation**: How the network adjusts weights and biases by calculating gradients of the loss.\n",
        "\n",
        "### Example of a single-layer neural network pass:\n",
        "$$\n",
        "y = W x + b\n",
        "$$\n",
        "If $x$ is a 3-dimensional input, $W$ might be a $2 \\times 3$ matrix, and $b$ a 2-dimensional vector—thus the output $y$ is 2-dimensional.\n"
      ],
      "id": "oxaMz35dOpm2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C49aY5NOpm2"
      },
      "source": [
        "## **8. Step by Step Example: Building a Tiny Neural Network from Scratch**\n",
        "We’ll create a toy example in PyTorch that:\n",
        "1. Takes an input matrix (like our feature set),\n",
        "2. Multiplies by a weight matrix, adds bias,\n",
        "3. Uses a ReLU activation,\n",
        "4. Computes a simple loss function,\n",
        "5. Performs a gradient descent update.\n"
      ],
      "id": "5C49aY5NOpm2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmCxRWXGOpm2"
      },
      "source": [
        "### **Illustrative Problem**\n",
        "Let's say we have 2D input data (two features per data point) and we want to predict a single output (like a regression problem). We'll create random data and see how the network learns.\n",
        "\n",
        "### **Real World Problem**\n",
        "This could correspond to something like predicting the price of a product based on two features: weight and height, or predicting a student’s test score based on two features: hours studied and hours slept, etc. The concept is the same.\n"
      ],
      "id": "zmCxRWXGOpm2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awKKHrYvOpm2"
      },
      "source": [
        "## **9. Implementation in PyTorch**\n",
        "We’ll do a small demonstration. We'll create a random dataset, define a single-layer network, define a loss, and run a few steps of gradient descent.\n"
      ],
      "id": "awKKHrYvOpm2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llnsuIg9Opm3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50e34a36-e5a0-4890-ac3b-a9568dd8d259"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Generate some random data\n",
        "X = torch.randn(100, 2)  # 100 data points, each with 2 features\n",
        "# Let's define a ground truth linear model: y = 3*x1 - 2*x2 + 1\n",
        "true_W = torch.tensor([[3.0], [-2.0]])  # shape (2,1)\n",
        "true_b = 1.0\n",
        "y = X @ true_W + true_b  # shape (100, 1)\n",
        "\n",
        "# We'll create a simple linear model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 1),  # from 2 input features to 1 output\n",
        ")\n",
        "\n",
        "# Define a loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop (gradient descent)\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(X)\n",
        "    loss = criterion(predictions, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Let's inspect the learned parameters\n",
        "[W_learned, b_learned] = list(model.parameters())\n",
        "print('Learned Weights:', W_learned)\n",
        "print('Learned Bias:', b_learned)\n",
        "\n",
        "print(f\"Final Loss: {loss.item():.4f}\")"
      ],
      "execution_count": null,
      "id": "llnsuIg9Opm3",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Weights: Parameter containing:\n",
            "tensor([[ 2.9117, -1.8449]], requires_grad=True)\n",
            "Learned Bias: Parameter containing:\n",
            "tensor([0.8718], requires_grad=True)\n",
            "Final Loss: 0.0310\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIQpbQbuOpm3"
      },
      "source": [
        "> **Explanation**: Over the training, the network adjusts its weights and bias to match the pattern in the data (\\(y = 3x_1 - 2x_2 + 1\\)). Ideally, after enough iterations, we should get close to `[3, -2]` for the weights and `1` for the bias."
      ],
      "id": "xIQpbQbuOpm3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdecyIbdOpm3"
      },
      "source": [
        "## **10. Additional Questions (Points to Ponder)**\n",
        "1. What if you had 3D input data or 100D input data? How do matrix dimensions change?\n",
        "2. How does adding more layers (more matrix multiplications) help the network learn more complex patterns?\n",
        "3. What is the role of activation functions in ensuring the network can learn non-linear relationships?\n",
        "4. How would you extend this to classification problems (e.g., cat vs dog images)?\n",
        "5. What happens if you remove biases from the network?\n",
        "6. How do you interpret eigenvalues and eigenvectors in the context of data transformations?\n",
        "7. Why do we need GPUs for large matrix multiplications?\n"
      ],
      "id": "WdecyIbdOpm3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzvLHq9dOpm3"
      },
      "source": [
        "## **11. Answers & Explanations**\n",
        "1. **Higher-dimensional input**: If you have more features, your weight matrix changes shape. For example, with 3D input and 2D output, the weight matrix is \\(2\\times3\\).\n",
        "2. **Deeper networks**: Stacking more layers adds more transformations, giving the network the ability to learn more complex functions.\n",
        "3. **Activation functions**: Without activations, multiple linear transformations would collapse into a single linear transformation. Activations create non-linearity, allowing the network to capture complex relationships.\n",
        "4. **Classification**: You’d typically use a softmax layer at the end and a cross-entropy loss. The matrix dimension for the last layer equals the number of classes.\n",
        "5. **No biases**: The model might be forced through the origin, possibly limiting its ability to fit data with offsets.\n",
        "6. **Eigenvalues/eigenvectors**: In data transformations (e.g., PCA), the eigenvectors of the covariance matrix represent principal directions. In transformations, they indicate directions that don’t change, only scale.\n",
        "7. **GPUs**: They are highly optimized for parallel computations, such as large-scale matrix multiplications. Without GPUs, training large networks would be prohibitively slow.\n",
        "\n",
        "---"
      ],
      "id": "qzvLHq9dOpm3"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S5gxZ71a5-4B"
      },
      "id": "S5gxZ71a5-4B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8Xj0uHuOpm3"
      },
      "source": [
        "## **12. An Illustrative Example**\n",
        "\n",
        "This snippet focuses on **matrix multiplication** and **eigen decomposition** in PyTorch.\n"
      ],
      "id": "H8Xj0uHuOpm3"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Fill in the missing parts (look for the TODO comments)\n",
        "import torch\n",
        "\n",
        "# TODO 1: Create a 3x3 matrix named M with any numbers you like\n",
        "M = torch.tensor([[1, 3, 5],\n",
        "                  [7, 9, 12],\n",
        "                  [45, 89, 32]], dtype=torch.float)\n",
        "\n",
        "# TODO 2: Print M\n",
        "print('Matrix M:\\n', M)\n",
        "\n",
        "# TODO 3: Create another 3x3 matrix N\n",
        "N = torch.tensor([[12, 90, 23],\n",
        "                  [34, 65, 45],\n",
        "                  [56, 78, 12]], dtype=torch.float)\n",
        "\n",
        "# Multiply M and N\n",
        "MN = torch.matmul(M, N)\n",
        "print('\\nM x N = \\n', MN)\n",
        "\n",
        "# TODO 4: Compute the eigenvalues and eigenvectors of M\n",
        "vals, vecs = torch.linalg.eig(M)\n",
        "print('\\nEigenvalues of M:', vals)\n",
        "print('Eigenvectors of M:\\n', vecs)\n",
        "\n",
        "# TODO 5: Verify the first eigenpair. (We already did this above, replicate the logic)\n",
        "v_first = vecs[:,0]\n",
        "lambda_first = vals[0]\n",
        "Mv = torch.matmul(M.type(torch.cfloat), v_first)\n",
        "lambda_v = lambda_first * v_first\n",
        "print('\\nCheck M * v vs lambda_v * v')\n",
        "print('M * v =', Mv)\n",
        "print('lambda_v * v =', lambda_v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YG294Ij50sd",
        "outputId": "e465fd8d-eab0-4ebb-cf2a-f3ebea5fd290"
      },
      "id": "6YG294Ij50sd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix M:\n",
            " tensor([[ 1.,  3.,  5.],\n",
            "        [ 7.,  9., 12.],\n",
            "        [45., 89., 32.]])\n",
            "\n",
            "M x N = \n",
            " tensor([[  394.,   675.,   218.],\n",
            "        [ 1062.,  2151.,   710.],\n",
            "        [ 5358., 12331.,  5424.]])\n",
            "\n",
            "Eigenvalues of M: tensor([ 59.0435+0.j,  -1.3584+0.j, -15.6851+0.j])\n",
            "Eigenvectors of M:\n",
            " tensor([[ 0.0958+0.j,  0.9027+0.j, -0.2018+0.j],\n",
            "        [ 0.2448+0.j, -0.3830+0.j, -0.3813+0.j],\n",
            "        [ 0.9648+0.j, -0.1960+0.j,  0.9021+0.j]])\n",
            "\n",
            "Check M * v vs lambda_v * v\n",
            "M * v = tensor([ 5.6543+0.j, 14.4513+0.j, 56.9677+0.j])\n",
            "lambda_v * v = tensor([ 5.6543+0.j, 14.4513+0.j, 56.9678+0.j])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tTbBUxmR5xTO"
      },
      "id": "tTbBUxmR5xTO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2MTHscEOpm3"
      },
      "source": [
        "This simple exercise helps reinforce the following concepts:\n",
        "- Creating matrices in PyTorch.\n",
        "- Performing matrix multiplication.\n",
        "- Calculating eigenvalues and eigenvectors.\n",
        "- Verifying the eigen decomposition manually.\n",
        "\n",
        "---"
      ],
      "id": "u2MTHscEOpm3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSuTPnq4Opm3"
      },
      "source": [
        "## **13. Glossary**\n",
        "- **Matrix**: A 2D array of numbers.\n",
        "- **Matrix Multiplication (matmul)**: A specific rule for multiplying 2D arrays of compatible shapes.\n",
        "- **Vector**: A 1D array of numbers.\n",
        "- **Eigenvector**: A special vector that only gets scaled (not rotated) by a matrix transformation.\n",
        "- **Eigenvalue**: The factor by which an eigenvector is scaled.\n",
        "- **Weight Matrix**: The parameters in a neural network layer that multiply the input.\n",
        "- **Bias**: An additive constant in each layer.\n",
        "- **Activation Function**: A non-linear function applied after linear transformations (e.g., ReLU, Sigmoid).\n",
        "- **Loss Function**: A measure of how well the model’s predictions match the true labels.\n",
        "- **Gradient Descent**: An optimization algorithm that adjusts parameters to reduce the loss.\n",
        "- **PyTorch**: A popular deep learning framework that uses tensors for computation.\n",
        "\n",
        "---\n",
        "## **Conclusion**\n",
        "We've walked through linear algebra fundamentals, matrix operations, and how they apply to neural networks—from the ground up to advanced research levels. Understanding these core concepts is crucial to effectively building and analyzing modern AI systems. **Happy coding and exploring!**\n",
        "\n",
        "---\n",
        "**End of Notebook**"
      ],
      "id": "oSuTPnq4Opm3"
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, platform, datetime, uuid, socket\n",
        "\n",
        "def signoff(name=\"Anonymous\"):\n",
        "    colab_check = \"Yes\" if 'google.colab' in sys.modules else \"No\"\n",
        "    mac_addr = ':'.join(format((uuid.getnode() >> i) & 0xff, '02x') for i in reversed(range(0, 48, 8)))\n",
        "    print(\"+++ Acknowledgement +++\")\n",
        "    print(f\"Executed on: {datetime.datetime.now()}\")\n",
        "    print(f\"In Google Colab: {colab_check}\")\n",
        "    print(f\"System info: {platform.system()} {platform.release()}\")\n",
        "    print(f\"Node name: {platform.node()}\")\n",
        "    print(f\"MAC address: {mac_addr}\")\n",
        "    try:\n",
        "        print(f\"IP address: {socket.gethostbyname(socket.gethostname())}\")\n",
        "    except:\n",
        "        print(\"IP address: Unknown\")\n",
        "    print(f\"Signing off, {name}\")\n",
        "\n",
        "signoff(\"Kushal Chandani\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1DbIeY4PJzi",
        "outputId": "939fcfb9-6e6b-4d7b-cd21-762be0c47f62"
      },
      "id": "o1DbIeY4PJzi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++ Acknowledgement +++\n",
            "Executed on: 2025-01-31 16:04:17.815939\n",
            "In Google Colab: Yes\n",
            "System info: Linux 6.1.85+\n",
            "Node name: e1e2be6ce382\n",
            "MAC address: 02:42:ac:1c:00:0c\n",
            "IP address: 172.28.0.12\n",
            "Signing off, Kushal Chandani\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fAycIGedGxUP"
      },
      "id": "fAycIGedGxUP",
      "execution_count": null,
      "outputs": []
    }
  ]
}