{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "name": "transformers_with_dr_adnan_masood",
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e88eO1eLs-GB"
      },
      "source": [
        "#################################################################\n",
        "#                                                               #\n",
        "#  CS435 Generative AI: Security, Ethics and Governance         #\n",
        "#                                                               #\n",
        "#  Instructor: Dr. Adnan Masood                                 #\n",
        "#  Contact:    adnanmasood@gmail.com                            #\n",
        "#                                                               #\n",
        "#  Notebook is MIT Licensed                                     #\n",
        "#################################################################"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e88eO1eLs-GB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsvtjrfTs-GD"
      },
      "source": [
        "# Transformers with Dr. Adnan Masood\n",
        "\n",
        "Welcome to this comprehensive Jupyter Notebook on Transformers, Large Language Models (LLMs), Neural Networks, and Generative AI!\n",
        "\n",
        "In this notebook, we will:\n",
        "1. **Explain Transformers in Building an Intuitive Understanding** (from a middle-school-friendly explanation all the way up to a PhD-level overview).\n",
        "2. **Explain the intuition** behind Transformer technology.\n",
        "3. **Briefly cover the history**, invention, and underlying tech.\n",
        "4. **Explain the math** behind Transformers, building on each layer step by step.\n",
        "5. **Provide an illustrative example** with code.\n",
        "6. **Do mock calculations** and define relevant terms (weights, bias, etc.).\n",
        "7. **Create a step-by-step example** of how to build a Transformer-like model from scratch.\n",
        "8. **Show an illustrative problem** it solves.\n",
        "9. **Show a real-world problem** it solves.\n",
        "10. **Demonstrate** how we can tackle a real-world problem using this tech.\n",
        "11. **Provide questions** to illustrate the use of this tech, along with answers and code examples.\n",
        "12. **Present an easy code sample** with TODO items and hints that students can complete.\n",
        "13. **Give a brief glossary** of related terms at the end.\n",
        "\n",
        "We'll be using **PyTorch** to illustrate the concepts with code.\n",
        "\n",
        "In true Dr. Adnan Masood style, let's get started!\n"
      ],
      "id": "bsvtjrfTs-GD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i-A_d99s-GD"
      },
      "source": [
        "# Chapter 9: The Transformer (Full Content)\n",
        "\n",
        "Below is the **entire chapter** (Chapter 9 from _Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2024. All rights reserved. Draft of January 12, 2025._) which forms the basis for our discussion and examples:\n",
        "\n",
        "---\n",
        "\n",
        "## CHAPTER 9  \n",
        "**The Transformer**\n",
        "\n",
        "> *“The true art of memory is the art of attention”*  \n",
        "> — Samuel Johnson, Idler #74, September 1759\n",
        "\n",
        "In this chapter we introduce the transformer, the standard architecture for building large language models. Transformer-based large language models have completely changed the field of speech and language processing. Indeed, every subsequent chapter in this textbook will make use of them. We’ll focus for now on left-to-right (sometimes called *causal* or *autoregressive*) language modeling, in which we are given a sequence of input tokens and predict output tokens one by one by conditioning on the prior context.\n",
        "\n",
        "The transformer is a neural network with a specific structure that includes a mechanism called **self-attention** or **multi-head attention**.^1 Attention can be thought of as a way to build contextual representations of a token’s meaning by attending to and integrating information from surrounding tokens, helping the model learn how tokens relate to each other over large spans.\n",
        "\n",
        "---\n",
        "1. Although multi-head attention developed historically from the RNN attention mechanism (Chapter 8), we’ll define attention from scratch here for readers who haven’t yet read Chapter 8.\n",
        "---\n",
        "\n",
        "### Stacked Transformer Blocks\n",
        "\n",
        "Transformer-based language models are complex, and so the details will unfold over the next 5 chapters. In the next sections we’ll introduce multi-head attention, the rest of the transformer block, and the input encoding and language modeling head components. Chapter 10 discusses how language models are pretrained, and how tokens are generated via sampling. Chapter 11 introduces masked language modeling and the BERT family of bidirectional transformer encoder models. Chapter 12 shows how to prompt LLMs to perform NLP tasks by giving instructions and demonstrations, and how to align the model with human preferences. Chapter 13 will introduce machine translation with the encoder-decoder architecture.\n",
        "\n",
        "## 9.1 Attention\n",
        "\n",
        "Recall from Chapter 6 that for word2vec and other static embeddings, the representation of a word’s meaning is always the same vector irrespective of the context: the word *chicken*, for example, is always represented by the same fixed vector. So a static vector for the word *it* might somehow encode that this is a pronoun used for animals and inanimate entities. But in context it has a much richer meaning.\n",
        "\n",
        "Consider *it* in one of these two sentences:\n",
        "\n",
        "1. The chicken didn’t cross the road because it was too tired.  \n",
        "2. The chicken didn’t cross the road because it was too wide.\n",
        "\n",
        "In (1) *it* is the chicken (i.e., the reader knows that the chicken was tired), while in (2) *it* is the road (and the reader knows that the road was wide).^2\n",
        "\n",
        "That is, if we are to compute the meaning of this sentence, we’ll need the meaning of *it* to be associated with the *chicken* in the first sentence and associated with the *road* in the second one, sensitive to the context. Furthermore, consider reading left to right like a causal language model, processing the sentence up to the word *it*:\n",
        "\n",
        "> The chicken didn’t cross the road because it\n",
        "\n",
        "At this point we don’t yet know which thing *it* is going to end up referring to! So a representation of *it* at this point might have aspects of both *chicken* and *road* as the reader is trying to guess what happens next.\n",
        "\n",
        "This fact that words have rich linguistic relationships with other words that may be far away pervades language. Consider two more examples:\n",
        "\n",
        "1. The keys to the cabinet are on the table.  \n",
        "2. I walked along the pond, and noticed one of the trees along the bank.\n",
        "\n",
        "In (1), the phrase *The keys* is the subject of the sentence, and in English and many languages, must agree in grammatical number with the verb *are*; in this case both are plural. In English we can’t use a singular verb like *is* with a plural subject like *keys* (we’ll discuss agreement more in Chapter 18). In (2), we know that *bank* refers to the side of a pond or river and not a financial institution because of the context, including words like *pond*. (We’ll discuss word senses more in Chapter 11.)\n",
        "\n",
        "The point of all these examples is that these contextual words that help us compute the meaning of words in context can be quite far away in the sentence or paragraph. Transformers can build contextual representations of word meaning, *contextual embeddings*, by integrating the meaning of these helpful contextual words. In a transformer, layer by layer, we build up richer and richer contextualized representations of the meanings of input tokens.\n",
        "\n",
        "### 9.1.1 Attention more formally\n",
        "\n",
        "Attention is the mechanism in the transformer that weighs and combines the representations from appropriate other tokens in the context from layer $k-1$ to layer $k$. A simplified version of the attention is:\n",
        "\n",
        "$$\n",
        "a_i = \\sum_{j \\le i} \\alpha_{ij} x_j\n",
        "$$\n",
        "\n",
        "where $\\alpha_{ij}$ is how much $x_j$ should contribute to $a_i$. In attention, we weight each prior embedding proportionally to how similar it is to the current token $x_i$. We compute similarity scores via dot product, normalize with softmax, etc.\n",
        "\n",
        "#### A single attention head using query, key, and value matrices\n",
        "\n",
        "Transformers use multiple weight matrices $W^Q$, $W^K$, $W^V$ to transform each input vector $x_i$ into **query**, **key**, and **value** representations. Then we compute dot products of query vs. key, scale them, apply softmax, and use them to weight the values:\n",
        "\n",
        "$$\n",
        "q_i = x_i W^Q; \\quad k_j = x_j W^K; \\quad v_j = x_j W^V;\n",
        "$$\n",
        "$$\n",
        "score(x_i, x_j) = \\frac{q_i \\cdot k_j}{\\sqrt{d_k}};\n",
        "$$\n",
        "$$\n",
        "\\alpha_{ij} = softmax(score(x_i, x_j)) \\quad \\forall j \\le i;\n",
        "$$\n",
        "$$\n",
        "head_i = \\sum_{j \\le i} \\alpha_{ij} v_j; \\quad a_i = head_i W^O.\n",
        "$$\n",
        "\n",
        "In **multi-head attention**, we repeat this process with multiple heads, each with its own $W^Q, W^K, W^V$. Then we concatenate the outputs of each head and project them back to dimension $d$ with another weight matrix.\n",
        "\n",
        "## 9.2 Transformer Blocks\n",
        "\n",
        "The self-attention calculation lies at the core of what’s called a transformer block, which, in addition to the self-attention layer, includes three other kinds of layers: (1) a feedforward layer, (2) residual connections, and (3) normalizing layers (layer norm).\n",
        "\n",
        "### Layer Norm\n",
        "\n",
        "Layer normalization ($LayerNorm$) helps keep the values of hidden layers in a stable range for gradient-based training. For a given vector $x \\in \\mathbb{R}^d$:\n",
        "\n",
        "$$\n",
        "\\mu = \\frac{1}{d} \\sum_{i=1}^{d} x_i, \\quad\n",
        "\\sigma = \\sqrt{\\frac{1}{d} \\sum_{i=1}^{d}(x_i - \\mu)^2}.\n",
        "$$\n",
        "Then\n",
        "$$\n",
        "\\hat{x} = \\frac{x - \\mu}{\\sigma}, \\quad LayerNorm(x) = \\gamma \\hat{x} + \\beta.\n",
        "$$\n",
        "\n",
        "### Feedforward Layer\n",
        "\n",
        "$$\n",
        "FFN(x) = ReLU(xW_1 + b_1)W_2 + b_2.\n",
        "$$\n",
        "\n",
        "### Putting it all together\n",
        "\n",
        "The function computed by a transformer block can be expressed as:\n",
        "\n",
        "$$\n",
        "T_1 = LayerNorm(X), \\quad\n",
        "T_2 = MultiHeadAttention(T_1), \\quad\n",
        "T_3 = T_2 + X,\n",
        "$$\n",
        "$$\n",
        "T_4 = LayerNorm(T_3), \\quad\n",
        "T_5 = FFN(T_4), \\quad\n",
        "H = T_5 + T_3.\n",
        "$$\n",
        "\n",
        "We stack many such transformer blocks for large language models (12, 24, 96, etc.).\n",
        "\n",
        "## 9.3 Parallelizing computation using a single matrix X\n",
        "\n",
        "Instead of processing one token at a time, we pack input embeddings for the $N$ tokens of the input into a single matrix $X \\in \\mathbb{R}^{N\\times d}$. This lets us efficiently compute attention using matrix multiplication.\n",
        "\n",
        "## 9.4 The input: embeddings for token and position\n",
        "\n",
        "To represent inputs, we combine **word embeddings** (from an embedding matrix $E$) and **positional embeddings** to form $X \\in \\mathbb{R}^{N\\times d}$. For left-to-right language models, we also add a **causal mask** so that each token can only attend to tokens at earlier positions.\n",
        "\n",
        "## 9.5 The Language Modeling Head\n",
        "\n",
        "Finally, for language modeling, we have a **language modeling head** on top of the final transformer block output. Often we *tie* the unembedding matrix to the embedding matrix $E$ by using its transpose. If $h_N^L$ is the final hidden state for the last token, then the logits $u$ and word probabilities $y$ are:\n",
        "\n",
        "$$\n",
        "u = h_N^L E^T, \\quad y = softmax(u).\n",
        "$$\n",
        "\n",
        "We then sample (or otherwise decode) from these probabilities.\n",
        "\n",
        "## 9.6 Summary\n",
        "\n",
        "- Transformers use **multi-head self-attention** to combine information from across the input.\n",
        "- A **transformer block** has self-attention, a feedforward layer, residual connections, and layer norms.\n",
        "- For language modeling, we use a **decoder-only** transformer that is masked so each token can only attend to previous tokens.\n",
        "\n",
        "## Bibliographical and Historical Notes\n",
        "\n",
        "The transformer (Vaswani et al., 2017) was developed from earlier concepts of self-attention and memory networks. The concept of attention originated in RNN-based sequence-to-sequence models (Bahdanau et al., 2015), then extended to self-attention (Cheng et al., 2016). Other aspects derived from memory networks (Weston et al., 2015; Graves et al., 2014).\n",
        "\n",
        "---\n",
        "2. We say that *it* corefers with *chicken* in the first example and with *road* in the second. We return to coreference in Chapter 23.\n",
        "---\n"
      ],
      "id": "0i-A_d99s-GD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz2bMjSUs-GE"
      },
      "source": [
        "# Building an Intuitive Understanding\n",
        "\n",
        "We will now explain **Transformers** in different levels of detail. Pick the explanation that resonates with your current understanding, and feel free to read them all for a layered perspective.\n"
      ],
      "id": "Wz2bMjSUs-GE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk3cyKAAs-GE"
      },
      "source": [
        "Imagine you’re reading a story. Each word in the story needs to know about the words before it to make sense of what is happening. A **Transformer** is like a big helper that looks at all the words you’ve already read and decides which ones are most important to understanding the next word. It does this using a trick called **attention**, which helps the model “focus” on the right words when figuring out the meaning of the current word. By doing this many times, Transformers learn to predict and create text that sounds just like people wrote it."
      ],
      "id": "nk3cyKAAs-GE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvhRg9Bss-GE"
      },
      "source": [
        "A Transformer takes a sentence (or a large chunk of text) and turns each word into a bunch of numbers (an *embedding*). Then, it uses something called **self-attention** to see how each word connects to every other word. This way, the model isn’t just looking at neighbors (like a small window around a word), it can look at the *whole sentence or paragraph* at once. This self-attention step is repeated in layers, each time refining how the words relate to each other. In the end, the model learns a powerful representation of the text that lets it do tasks like predicting the next word or completing entire sentences."
      ],
      "id": "uvhRg9Bss-GE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEKLe26is-GF"
      },
      "source": [
        "Transformers represent each token (word or subword) in a vector space. They compute **query**, **key**, and **value** vectors for each token in parallel. The **queries** are used to look up relevant **keys** across all tokens, and the corresponding **values** are combined based on similarity scores. This process, known as **multi-head self-attention**, is repeated several times with **feedforward layers**, **residual connections**, and **layer normalization**. By stacking multiple Transformer blocks, the model captures global context and learns intricate linguistic patterns. The final layer, known as the **language modeling head**, transforms these representations into a distribution over possible next tokens (words)."
      ],
      "id": "fEKLe26is-GF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyJEqndms-GF"
      },
      "source": [
        "A Transformer encodes each input token $x_i$ into a vector. Within each Transformer block, a **multi-head self-attention** layer is applied: each head has distinct learnable weight matrices $W^Q, W^K, W^V$ to create query ($q_i$), key ($k_j$), and value ($v_j$) vectors. We compute dot products $q_i \\cdot k_j$, scale them by $\\sqrt{d_k}$, apply a softmax, and multiply by $v_j$ to get the final attention output. Multiple heads are concatenated and projected to maintain dimension consistency. This output feeds into a **feedforward network** with potential dimension expansion. **Residual connections** add the input of each sub-layer to its output, and **layer normalization** maintains stable activations. Stacking multiple such layers yields a deep, contextualized representation of all tokens. Positional embeddings or encodings are added to represent sequence order. For language modeling, a final projection (often tied to the embedding matrix) provides logits over the vocabulary."
      ],
      "id": "gyJEqndms-GF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9oLO8g3s-GF"
      },
      "source": [
        "The Transformer can be viewed as a composition of multiple learned functions on top of an initial embedded input $X \\in \\mathbb{R}^{N\\times d}$. Each **Transformer block** implements:\n",
        "\n",
        "$$\n",
        "T_1 = LayerNorm(X),  \\quad\n",
        "T_2 = MultiHeadAttention(T_1),  \\quad\n",
        "T_3 = T_2 + X,\n",
        "$$\n",
        "$$\n",
        "T_4 = LayerNorm(T_3),  \\quad\n",
        "T_5 = FFN(T_4),        \\quad\n",
        "H   = T_5 + T_3.\n",
        "$$\n",
        "\n",
        "We can interpret multi-head attention as multiple parallel subspaces focusing on distinct relational patterns. The attention matrix is $\\mathrm{softmax}((X W^Q)(X W^K)^T /\\sqrt{d_k})$. The feedforward network introduces a non-linear transformation (often ReLU) with dimension expansion to $d_{ff}$, then back to $d$. These blocks are repeated $L$ times to form deeper models. Causal (left-to-right) masking ensures that the model cannot attend to future tokens, preserving autoregressive capabilities. Output embeddings are mapped back to vocabulary logits via weight tying with $E^T$. This architecture’s parallelizable design yields remarkable efficiency and expressivity, forming the backbone of modern large language models such as GPT variants."
      ],
      "id": "l9oLO8g3s-GF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPEAzWDSs-GF"
      },
      "source": [
        "# Intuition Behind the Technology\n",
        "\n",
        "The **key idea** behind Transformers is **attention**:\n",
        "- Instead of looking at text in a strict left-to-right manner (like older recurrent networks), or in fixed windows (like convolutional networks), the Transformer can look at every token in a sequence (or a chunk of the sequence) in relation to every other token.\n",
        "- This means each word can gather meaning from all relevant context words at once.\n",
        "- Multi-head attention means multiple ways of looking at these relationships.\n",
        "\n",
        "This simultaneously addresses the need for long-range context (the model can easily connect distant parts of a paragraph) and the bottleneck of sequential processing (the Transformer architecture is parallelizable)."
      ],
      "id": "BPEAzWDSs-GF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F4cMOUJs-GG"
      },
      "source": [
        "# Brief History, Invention, and Underlying Tech\n",
        "\n",
        "Historically:\n",
        "- **Attention** first appeared in RNN-based sequence models for machine translation (Bahdanau et al., 2015), allowing the decoder to look back at relevant portions of the input.\n",
        "- The idea of **self-attention** was explored to let each token attend to the rest of the sequence.\n",
        "- The **Transformer** was introduced by Vaswani et al. (2017) in the seminal paper *\"Attention Is All You Need\"*. It got rid of recurrent and convolutional structures entirely and relied solely on attention mechanisms.\n",
        "\n",
        "The underlying technology includes:\n",
        "- **Dot-product attention** for similarity computation.\n",
        "- **Feedforward layers** for non-linear transformations.\n",
        "- **Positional embeddings** to handle the ordering of tokens.\n",
        "- **Residual connections** and **layer normalization** to facilitate training of deep networks.\n",
        "- Parallel processing enabled by representing the entire sequence as a matrix.\n"
      ],
      "id": "6F4cMOUJs-GG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlK9iKfYs-GG"
      },
      "source": [
        "# The Math Behind Transformers\n",
        "\n",
        "1. **Input Representation**: Suppose you have a sequence of tokens (words) $[w_1, w_2, ..., w_N]$. Each token is turned into a numerical vector of size $d$ (the *embedding*). We place these row-wise into a matrix $X \\in \\mathbb{R}^{N\\times d}$.\n",
        "2. **Query, Key, Value**: For each token, we create three versions of its embedding:\n",
        "   - **Query**: $Q = XW^Q$\n",
        "   - **Key**: $K = XW^K$\n",
        "   - **Value**: $V = XW^V$\n",
        "   where $W^Q, W^K, W^V$ are learnable matrices, and the resulting $Q, K, V$ each have shape $(N\\times d_k), (N\\times d_k), (N\\times d_v)$ respectively.\n",
        "3. **Attention Weights**: Compute the similarity of every query with every key by multiplying $QK^T$. This gives an $(N\\times N)$ matrix of similarity scores. Scale by $\\sqrt{d_k}$ and apply a softmax, yielding an attention matrix $\\alpha$ of shape $(N\\times N)$.\n",
        "4. **Weighted Sum of Values**: Multiply $\\alpha$ by $V$ to combine the value vectors in proportion to their relevance.\n",
        "5. **Multi-head**: Repeat the above process with multiple sets of $(W^Q, W^K, W^V)$ to capture different aspects of the input. Concatenate all the heads’ outputs and project back to dimension $d$.\n",
        "6. **Feedforward + Residual + LayerNorm**: For each token’s output, apply a small 2-layer MLP, add a residual connection, and do a layer normalization.\n",
        "7. **Stack** the above block multiple times. Finally, use a linear + softmax (the **language modeling head**) to produce probabilities over possible next tokens.\n"
      ],
      "id": "BlK9iKfYs-GG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F0uYloTs-GG"
      },
      "source": [
        "# Intuitive Example + Code\n",
        "\n",
        "Let’s illustrate the self-attention idea with code in a simple setting. We’ll create a tiny vocabulary of words and do a toy demonstration.\n"
      ],
      "id": "6F0uYloTs-GG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvax9iDis-GG",
        "outputId": "4784bb87-8e91-4bc5-e0af-3b8492096d25"
      },
      "source": [
        "# Let's do a small demonstration with a few tokens.\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Suppose we have an input of 3 tokens, each mapped to an embedding dim=4 (just for demonstration)\n",
        "N = 3  # number of tokens\n",
        "d = 4  # embedding dimension\n",
        "\n",
        "# Example random embeddings for each token\n",
        "x = torch.randn(N, d)\n",
        "print('Input embedding matrix x (shape: 3x4):')\n",
        "print(x)\n",
        "\n",
        "# We'll create separate W^Q, W^K, W^V for a single head\n",
        "d_k = d_v = 4  # for simplicity, keep them the same as d in this tiny example\n",
        "W_Q = torch.randn(d, d_k)\n",
        "W_K = torch.randn(d, d_k)\n",
        "W_V = torch.randn(d, d_v)\n",
        "\n",
        "# Step 1: compute Q, K, V\n",
        "Q = x @ W_Q  # shape [3, 4]\n",
        "K = x @ W_K  # shape [3, 4]\n",
        "V = x @ W_V  # shape [3, 4]\n",
        "\n",
        "# Step 2: compute similarity scores QK^T\n",
        "scores = Q @ K.t()  # shape [3,3]\n",
        "\n",
        "# Step 3: scale by sqrt(d_k)\n",
        "scores = scores / (d_k ** 0.5)\n",
        "\n",
        "# Step 4: softmax to get attention weights\n",
        "alpha = F.softmax(scores, dim=-1)\n",
        "\n",
        "# Step 5: Weighted sum of V\n",
        "attn_out = alpha @ V  # shape [3, 4]\n",
        "\n",
        "print('\\nAttention weights (alpha):')\n",
        "print(alpha)\n",
        "print('\\nOutput of single-head self-attention (attn_out):')\n",
        "print(attn_out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input embedding matrix x (shape: 3x4):\n",
            "tensor([[-0.1240, -0.9095,  0.2444,  0.4379],\n",
            "        [ 0.5259,  1.7531, -0.8235, -1.4180],\n",
            "        [ 0.3587, -0.5939, -1.8652, -1.4063]])\n",
            "\n",
            "Attention weights (alpha):\n",
            "tensor([[0.0597, 0.1206, 0.8197],\n",
            "        [0.9244, 0.0720, 0.0036],\n",
            "        [0.0463, 0.0314, 0.9223]])\n",
            "\n",
            "Output of single-head self-attention (attn_out):\n",
            "tensor([[-1.8658, -3.7427, -2.3023, -0.5645],\n",
            "        [ 0.9894,  0.4736,  0.5466,  1.6118],\n",
            "        [-1.7904, -3.9063, -2.3876, -0.2353]])\n"
          ]
        }
      ],
      "id": "pvax9iDis-GG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhlegEfCs-GG"
      },
      "source": [
        "Above:\n",
        "1. We started with a random embedding matrix $x$ of shape $(3,4)$, representing 3 tokens, each mapped to 4 dimensions.\n",
        "2. We created random projection matrices for Query, Key, Value ($W^Q, W^K, W^V$).\n",
        "3. We computed **Q = xW^Q**, **K = xW^K**, **V = xW^V**.\n",
        "4. We calculated the dot-product between every Query and every Key ($QK^T$), scaled it, and then did a softmax to get our attention distribution.\n",
        "5. Finally, we did a weighted sum of the Value vectors, producing a new representation.\n",
        "\n",
        "This is the core of how a Transformer decides “which tokens to pay attention to.”"
      ],
      "id": "hhlegEfCs-GG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3LqUIEds-GG"
      },
      "source": [
        "# Example Calculations\n",
        "\n",
        "Let’s define some key terms often encountered in Transformer math:\n",
        "\n",
        "- **Weight (W)**: A learnable parameter matrix (like $W^Q$ in our code). Shapes typically match input dimensions and output dimensions.\n",
        "- **Bias (b)**: An additional learnable parameter (vector) sometimes used in feedforward layers (not shown in the above snippet, but used in standard linear layers).\n",
        "- **Dot Product**: The multiplication of two vectors, resulting in a scalar that indicates how similar or aligned they are.\n",
        "- **Softmax**: A function that turns a vector of real numbers into probabilities that sum to 1.\n",
        "- **Mask**: In causal language modeling, we mask out future tokens so the model doesn’t “cheat” by seeing them.\n",
        "\n",
        "### Example Calculation Step-by-step (Conceptual):\n",
        "1. **Token embedding**: The word \"Cat\" might be mapped to [0.2, 0.9, -0.1, 0.05].\n",
        "2. **Query projection**: Multiply [0.2, 0.9, -0.1, 0.05] by $W^Q$ (4x4 in our toy code) = new 4D vector.\n",
        "3. **Key projection**: Similarly, each token’s embedding is mapped by $W^K$.\n",
        "4. **Dot-product**: For the current token’s query, we do dot-products with each key vector, including itself.\n",
        "5. **Scale & Softmax**: Divide by $\\sqrt{d_k}$, apply softmax to get weights.\n",
        "6. **Weighted sum**: Multiply these weights by each token’s value vector, then sum.\n",
        "7. **Output**: This is the new representation of the current token, capturing context from other tokens."
      ],
      "id": "M3LqUIEds-GG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjTpWOcVs-GG"
      },
      "source": [
        "# Step by Step: Building the Technology from Scratch\n",
        "\n",
        "Here’s a **conceptual** 7-step approach to building a minimal Transformer-like language model **from scratch**:\n",
        "\n",
        "1. **Tokenization**: Decide how to split text into tokens (words, subwords, etc.) and map them to integer IDs.\n",
        "2. **Create Embeddings**: Initialize an embedding matrix $E$ with shape $(|V|, d)$. For each token, you get its vector by index lookup.\n",
        "3. **Positional Encoding**: Add position encodings (sine/cosine or learned) to each token’s embedding to incorporate order.\n",
        "4. **Multi-head Self-Attention**: Implement code that for each token, calculates $q, k, v$ vectors, does dot-product attention, and sums the results.\n",
        "5. **Feedforward Layer**: A small 2-layer MLP with a ReLU (or another activation) and possibly dimension expansion.\n",
        "6. **Residual Connections & Layer Norm**: Add skip connections and layer normalization around the attention and feedforward sub-layers.\n",
        "7. **Language Modeling Head**: Tie or create a new matrix to map final hidden states back to the vocabulary, produce a distribution via softmax.\n",
        "\n",
        "Repeat multiple times (stack more Transformer blocks) to get more capacity.\n"
      ],
      "id": "pjTpWOcVs-GG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmpYeuugs-GG"
      },
      "source": [
        "# Illustrative Problem It Solves\n",
        "\n",
        "A classic illustrative problem is **next-word prediction** or **text auto-completion**. By looking at the prior sequence of words, the Transformer predicts the *most likely* next token. This is the basis of technologies like GPT, ChatGPT, etc. If your input text is:\n",
        "\n",
        "> \"Neural networks are a class of models loosely inspired by biological...\"\n",
        "\n",
        "A Transformer-based language model can guess the next word might be \"brains,\" \"neurons,\" or \"neural processes,\" etc., ranking them by probability.\n"
      ],
      "id": "OmpYeuugs-GG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZStHYGQTs-GG"
      },
      "source": [
        "# Real World Problem It Solves\n",
        "\n",
        "One major real-world application of Transformers is **Machine Translation**. By using an *encoder-decoder* Transformer architecture (we only covered the decoder-only part here), you can translate from, say, French to English by learning to attend to relevant words in the French sentence for each word you generate in English.\n",
        "\n",
        "Another example is **summarization**: Transformers can condense large documents into shorter summaries while retaining the salient points.\n"
      ],
      "id": "ZStHYGQTs-GG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVdNkJuds-GG"
      },
      "source": [
        "# How to Solve a Real-World Problem (Text Summarization Example)\n",
        "\n",
        "1. **Collect Data**: Get a dataset of documents and reference summaries.\n",
        "2. **Build/Use a Transformer**: Use an encoder-decoder style or a large language model that can handle summarization tasks.\n",
        "3. **Train/Fine-tune**: If building from scratch, train the model on your data. If using a pre-trained model, fine-tune on your summarization dataset.\n",
        "4. **Inference**: For a new document, pass the text to the model. The model attends to the important parts of the text and generates a concise summary.\n",
        "\n",
        "Common frameworks: **HuggingFace Transformers**, **PyTorch Lightning**, etc.\n"
      ],
      "id": "FVdNkJuds-GG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hprCqz0Is-GG"
      },
      "source": [
        "# Questions to Illustrate the Use of This Tech\n",
        "Here are some prompts or questions you could ask to highlight the use of this tech:\n",
        "\n",
        "1. **\"How can a Transformer handle long-distance dependencies in text?\"**  \n",
        "   *Answer:* By using self-attention that can attend to any token in the sequence, not just local neighbors.\n",
        "\n",
        "2. **\"Why do we need positional embeddings if the model can see all tokens at once?\"**  \n",
        "   *Answer:* Because without some notion of order, the model wouldn’t know how the tokens are arranged in time/space.\n",
        "\n",
        "3. **\"What is multi-head attention?\"**  \n",
        "   *Answer:* Multiple attention heads allow the model to learn different types of relationships in parallel.\n",
        "\n",
        "4. **\"Why use residual connections and layer norm?\"**  \n",
        "   *Answer:* They help stabilize and speed up training, preventing vanishing or exploding gradients and allowing deeper architectures.\n",
        "\n",
        "5. **\"How is a Transformer different from an RNN?\"**  \n",
        "   *Answer:* An RNN processes tokens sequentially, which can be slower for long sequences, while a Transformer processes all tokens in parallel via attention.\n",
        "\n",
        "6. **\"What does the language modeling head do?\"**  \n",
        "   *Answer:* It projects the final hidden state of each token position into a distribution over the vocabulary, allowing next-token prediction.\n",
        "\n",
        "7. **\"How do we prevent a Transformer from looking at future tokens during training for language modeling?\"**  \n",
        "   *Answer:* Use a causal mask that sets attention weights to zero for tokens that occur after the current position.\n"
      ],
      "id": "hprCqz0Is-GG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4sG4BtIs-GG"
      },
      "source": [
        "# A Sample Exercise\n",
        "\n",
        "Below is a minimal PyTorch example for a small Transformer-based language model. Some parts are marked as **TODO** for you to complete!"
      ],
      "id": "Y4sG4BtIs-GG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Sd3UORvs-GG",
        "outputId": "e0690fbf-4d34-463c-93ed-83beca624de9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#######################################\n",
        "# HYPERPARAMETERS\n",
        "#######################################\n",
        "d_model = 16  # Embedding dimension\n",
        "num_tokens = 50  # Just a small vocab for demonstration\n",
        "max_seq_len = 10  # We'll limit sequence length for simplicity\n",
        "num_heads = 2   # Multi-head attention heads\n",
        "num_layers = 2  # Number of Transformer blocks\n",
        "batch_size = 4  # We'll process 4 sequences at a time\n",
        "\n",
        "#######################################\n",
        "# SAMPLE DATA (RANDOM)\n",
        "# In real life, you'd have a proper dataset of token IDs\n",
        "#######################################\n",
        "# We'll create random sequences of integer token IDs in [0, num_tokens)\n",
        "torch.manual_seed(42)\n",
        "dummy_inputs = torch.randint(0, num_tokens, (batch_size, max_seq_len))\n",
        "dummy_targets = torch.randint(0, num_tokens, (batch_size, max_seq_len))\n",
        "\n",
        "#######################################\n",
        "# POS EMBEDDING (LEARNED)\n",
        "#######################################\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        # Create a long enough positional embedding for all positions\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pe[:seq_len, :]\n",
        "        return x\n",
        "\n",
        "#######################################\n",
        "# BASIC TRANSFORMER BLOCK\n",
        "#######################################\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dim_feedforward=64):\n",
        "        super().__init__()\n",
        "        # TODO: create a nn.MultiheadAttention layer\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim_feedforward, d_model)\n",
        "        )\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        # Self-attention\n",
        "        attn_out, _ = self.attn(x, x, x)  # (batch_size, seq_len, d_model)\n",
        "        x = x + attn_out\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # Feedforward\n",
        "        ff_out = self.ff(x)\n",
        "        x = x + ff_out\n",
        "        x = self.norm2(x)\n",
        "        return x\n",
        "\n",
        "#######################################\n",
        "# MINI TRANSFORMER LANGUAGE MODEL\n",
        "#######################################\n",
        "class MiniTransformerLM(nn.Module):\n",
        "    def __init__(self, num_tokens, d_model, max_seq_len, num_heads=2, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.token_embed = nn.Embedding(num_tokens, d_model)\n",
        "        self.pos_embed = PositionalEncoding(d_model, max_len=max_seq_len)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Final linear layer to map d_model -> vocab size\n",
        "        self.output_lin = nn.Linear(d_model, num_tokens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len)\n",
        "        embed = self.token_embed(x)  # shape: (batch_size, seq_len, d_model)\n",
        "        out = self.pos_embed(embed)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            out = layer(out)\n",
        "\n",
        "        logits = self.output_lin(out)  # (batch_size, seq_len, num_tokens)\n",
        "        return logits\n",
        "\n",
        "# Create model\n",
        "model = MiniTransformerLM(num_tokens, d_model, max_seq_len, num_heads, num_layers)\n",
        "print(model)\n",
        "\n",
        "# Run a forward pass\n",
        "logits = model(dummy_inputs)\n",
        "print('Logits shape:', logits.shape)\n",
        "\n",
        "# Compare with targets\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# We'll flatten batch and sequence dims to compare with vocab\n",
        "logits_2d = logits.view(-1, num_tokens)\n",
        "targets_1d = dummy_targets.view(-1)\n",
        "loss = loss_fn(logits_2d, targets_1d)\n",
        "print('Loss:', loss.item())\n",
        "\n",
        "# Basic training step: backward\n",
        "loss.backward()\n",
        "# In real training, you'd update model params with an optimizer step\n",
        "# e.g.: optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MiniTransformerLM(\n",
            "  (token_embed): Embedding(50, 16)\n",
            "  (pos_embed): PositionalEncoding()\n",
            "  (layers): ModuleList(\n",
            "    (0-1): 2 x TransformerBlock(\n",
            "      (attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
            "      )\n",
            "      (ff): Sequential(\n",
            "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=64, out_features=16, bias=True)\n",
            "      )\n",
            "      (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (output_lin): Linear(in_features=16, out_features=50, bias=True)\n",
            ")\n",
            "Logits shape: torch.Size([4, 10, 50])\n",
            "Loss: 4.222940444946289\n"
          ]
        }
      ],
      "id": "5Sd3UORvs-GG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95HifHxos-GG"
      },
      "source": [
        "### TODO Items & Hints\n",
        "\n",
        "- **TODO**: Implement a causal mask to ensure the model only attends to previous positions. (Hint: in PyTorch’s `nn.MultiheadAttention`, you can pass an `attn_mask`.)\n",
        "- **TODO**: Add a proper training loop with an optimizer and multiple epochs.\n",
        "- **TODO**: Try a real text dataset, e.g. small samples of Wikipedia.\n",
        "- **HINT**: Investigate how you can tie the weights in the `output_lin` layer with the `token_embed` matrix if you want weight tying.\n"
      ],
      "id": "95HifHxos-GG"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "#######################################\n",
        "# HYPERPARAMETERS\n",
        "#######################################\n",
        "d_model = 16  # Embedding dimension\n",
        "num_tokens = 5000  # Vocabulary size\n",
        "max_seq_len = 10  # Maximum sequence length\n",
        "num_heads = 2   # Number of attention heads\n",
        "num_layers = 2  # Number of Transformer blocks\n",
        "batch_size = 4  # Batch size\n",
        "epochs = 10     # Number of training epochs\n",
        "learning_rate = 0.01  # Learning rate\n",
        "\n",
        "#######################################\n",
        "# DATASET PREPARATION\n",
        "#######################################\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, seq_len):\n",
        "        self.seq_len = seq_len\n",
        "        self.vocab = list(set(text))\n",
        "        self.char_to_idx = {char: idx for idx, char in enumerate(self.vocab)}\n",
        "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
        "        self.data = self.prepare_data(text)\n",
        "\n",
        "    def prepare_data(self, text):\n",
        "        sequences = []\n",
        "        for i in range(len(text) - self.seq_len):\n",
        "            seq = text[i:i + self.seq_len]\n",
        "            target = text[i + 1:i + self.seq_len + 1]\n",
        "            sequences.append((seq, target))\n",
        "        return sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, target = self.data[idx]\n",
        "        seq_idx = torch.tensor([self.char_to_idx[char] for char in seq], dtype=torch.long)\n",
        "        target_idx = torch.tensor([self.char_to_idx[char] for char in target], dtype=torch.long)\n",
        "        return seq_idx, target_idx\n",
        "\n",
        "# Fetch a small sample of Wikipedia text\n",
        "def fetch_wikipedia_sample():\n",
        "    url = 'https://en.wikipedia.org/wiki/DeepSeek'\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    paragraphs = soup.find_all('p')\n",
        "    text = ''.join([para.get_text() for para in paragraphs])\n",
        "    # Clean the text\n",
        "    text = re.sub(r'\\[[^\\]]*\\]', '', text)  # Remove references\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "    return text[:1000]  # Return a small sample\n",
        "\n",
        "# Prepare dataset\n",
        "text_sample = fetch_wikipedia_sample()\n",
        "dataset = TextDataset(text_sample, max_seq_len)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#######################################\n",
        "# POSITIONAL ENCODING\n",
        "#######################################\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pe[:seq_len, :]\n",
        "        return x\n",
        "\n",
        "#######################################\n",
        "# TRANSFORMER BLOCK WITH CAUSAL MASKING\n",
        "#######################################\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dim_feedforward=64):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim_feedforward, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        attn_out, _ = self.attn(x, x, x, attn_mask=attn_mask)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm2(x + ff_out)\n",
        "        return x\n",
        "\n",
        "#######################################\n",
        "# MINI TRANSFORMER LANGUAGE MODEL WITH WEIGHT TYING\n",
        "#######################################\n",
        "class MiniTransformerLM(nn.Module):\n",
        "    def __init__(self, num_tokens, d_model, max_seq_len, num_heads=2, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.token_embed = nn.Embedding(num_tokens, d_model)\n",
        "        self.pos_embed = PositionalEncoding(d_model, max_len=max_seq_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.output_lin = nn.Linear(d_model, num_tokens)\n",
        "        # Tie weights\n",
        "        self.output_lin.weight = self.token_embed.weight\n",
        "\n",
        "    def forward(self, x):\n",
        "        embed = self.token_embed(x)\n",
        "        out = self.pos_embed(embed)\n",
        "        # Create causal mask\n",
        "        seq_len = x.size(1)\n",
        "        attn_mask = torch.triu(torch.ones((seq_len, seq_len)), diagonal=1).bool()\n",
        "        attn_mask = attn_mask.to(x.device)\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, attn_mask=attn_mask)\n",
        "        logits = self.output_lin(out)\n",
        "        return logits\n",
        "\n",
        "# Initialize model\n",
        "model = MiniTransformerLM(len(dataset.vocab), d_model, max_seq_len, num_heads, num_layers)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "#######################################\n",
        "# TRAINING LOOP\n",
        "#######################################\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch in data_loader:\n",
        "        inputs, targets = batch\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(inputs)\n",
        "        loss = loss_fn(logits.view(-1, len(dataset.vocab)), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrPbRvD_tZ02",
        "outputId": "1d901562-8bd0-4a1f-b4a9-55b08fa23d04"
      },
      "id": "VrPbRvD_tZ02",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 3.5218\n",
            "Epoch 2/10, Loss: 2.7178\n",
            "Epoch 3/10, Loss: 2.3713\n",
            "Epoch 4/10, Loss: 2.1828\n",
            "Epoch 5/10, Loss: 2.0457\n",
            "Epoch 6/10, Loss: 1.9536\n",
            "Epoch 7/10, Loss: 1.8363\n",
            "Epoch 8/10, Loss: 1.7618\n",
            "Epoch 9/10, Loss: 1.7149\n",
            "Epoch 10/10, Loss: 1.6586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHI01Mo0s-GH"
      },
      "source": [
        "# Brief Glossary of Related Terms\n",
        "\n",
        "- **Attention**: A mechanism that computes a weighted combination of other elements in a sequence, based on similarity.\n",
        "- **Self-Attention**: Attention applied among tokens within the same sequence, allowing each token to attend to others.\n",
        "- **Multi-Head Attention**: Multiple parallel attention heads that each learn different relational patterns.\n",
        "- **Residual Connection**: Adding the input of a layer to its output to help training deeper models.\n",
        "- **Layer Normalization**: A method of normalizing the hidden layer for more stable and faster training.\n",
        "- **Feedforward Network (FFN)**: A small MLP (2-layer typically) inside each Transformer block.\n",
        "- **Causal Mask**: A mask preventing the model from attending to future tokens in language modeling.\n",
        "- **Embedding Matrix**: A matrix that maps token IDs to dense vectors.\n",
        "- **Weight Tying**: Reusing the same weight matrix for embedding and output layers (via transposition).\n",
        "- **Positional Embeddings**: Extra vectors added to token embeddings to encode position/order.\n",
        "- **Vocabulary (V)**: The set of all possible tokens (words, subwords) the model can output.\n",
        "- **Logits**: The raw, unnormalized scores over the vocabulary, transformed by a softmax to become probabilities.\n",
        "- **Causal (Decoder-Only) Transformer**: A Transformer that only looks backward in the sequence (past tokens) for language modeling tasks.\n"
      ],
      "id": "gHI01Mo0s-GH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJilI0Iks-GH",
        "outputId": "380ac937-41c3-4e40-817a-11af6bcb015e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os, sys, platform, datetime, uuid, socket\n",
        "\n",
        "def signoff(name=\"Anonymous\"):\n",
        "    colab_check = \"Yes\" if 'google.colab' in sys.modules else \"No\"\n",
        "    mac_addr = ':'.join(format((uuid.getnode() >> i) & 0xff, '02x') for i in reversed(range(0, 48, 8)))\n",
        "    print(\"+++ Acknowledgement +++\")\n",
        "    print(f\"Executed on: {datetime.datetime.now()}\")\n",
        "    print(f\"In Google Colab: {colab_check}\")\n",
        "    print(f\"System info: {platform.system()} {platform.release()}\")\n",
        "    print(f\"Node name: {platform.node()}\")\n",
        "    print(f\"MAC address: {mac_addr}\")\n",
        "    try:\n",
        "        print(f\"IP address: {socket.gethostbyname(socket.gethostname())}\")\n",
        "    except:\n",
        "        print(\"IP address: Unknown\")\n",
        "    print(f\"Signing off, {name}\")\n",
        "\n",
        "signoff(\"Kushal Chandani\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++ Acknowledgement +++\n",
            "Executed on: 2025-02-02 17:38:55.166593\n",
            "In Google Colab: Yes\n",
            "System info: Linux 6.1.85+\n",
            "Node name: e071de992219\n",
            "MAC address: 02:42:ac:1c:00:0c\n",
            "IP address: 172.28.0.12\n",
            "Signing off, Kushal Chandani\n"
          ]
        }
      ],
      "id": "VJilI0Iks-GH"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fMSllnGCvOW6"
      },
      "id": "fMSllnGCvOW6",
      "execution_count": null,
      "outputs": []
    }
  ]
}