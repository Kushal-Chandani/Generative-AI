{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1R1YQOdsSK6"
      },
      "source": [
        "#################################################################\n",
        "#                                                               #\n",
        "#  CS435 Generative AI: Security, Ethics and Governance         #\n",
        "#                                                               #\n",
        "#  Instructor: Dr. Adnan Masood                                 #\n",
        "#  Contact:    adnanmasood@gmail.com                            #\n",
        "#                                                               #\n",
        "#  Notebook is MIT Licensed                                     #\n",
        "#################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5sqVd2psSK7"
      },
      "source": [
        "# Transformers and Attention\n",
        "\n",
        "Welcome to today's lesson! In this notebook, we will explore **Transformers and Attention Mechanisms** in detail. This will include everything from basic intuition to building a simple transformer from scratch in PyTorch. We'll go step by step to understand the concepts, math, and code behind these technologies.\n",
        "\n",
        "---\n",
        "\n",
        "## Building an Intuitive Understanding\n",
        "To make it easier to grasp the material, we'll explain it at five levels:\n",
        "1. What is it and why does it matter?\n",
        "2. Where it came from and how it works at a high level.\n",
        "3. Understanding the math behind attention and transformers.\n",
        "4. Writing PyTorch code to implement and experiment.\n",
        "5. Building a transformer from scratch.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2or-8XCAsSK8"
      },
      "source": [
        "## 1. Attention in Transformers?\n",
        "Imagine reading a book and trying to summarize it. A transformer is like a super-smart assistant that not only reads the book but also pays attention to the important parts, understands their relationships, and creates a summary that's easy to understand.\n",
        "\n",
        "Why is this important? Because machines need to understand relationships between words to translate languages, generate text, or even answer questions like humans!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIyoGAVcsSK8"
      },
      "source": [
        "Transformers were introduced in 2017 in a research paper titled \"Attention is All You Need.\" Before transformers, models like RNNs and LSTMs struggled to handle long-term dependencies in text. Transformers solved this by using \"self-attention\" to focus on the most relevant parts of the input.\n",
        "\n",
        "Key ideas of transformers:\n",
        "1. **Self-Attention**: Helps the model focus on important words in a sentence.\n",
        "2. **Positional Encoding**: Adds information about the order of words.\n",
        "3. **Multi-Head Attention**: Looks at the input from multiple perspectives.\n",
        "4. **Feedforward Networks**: Processes attention outputs for prediction.\n",
        "5. **Layer Normalization**: Stabilizes training.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM67hBMfsSK8"
      },
      "source": [
        "## Understanding Attention\n",
        "Let's break down the self-attention mechanism step by step.\n",
        "\n",
        "### Self-Attention Formula\n",
        "The main idea is to compute a weighted sum of values where the weights are based on pairwise word similarity.\n",
        "For each word in a sentence, we compute:\n",
        "\n",
        "$$ Attention(Q, K, V) = Softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V $$\n",
        "\n",
        "Where:\n",
        "- $Q$: Query\n",
        "- $K$: Key\n",
        "- $V$: Value\n",
        "- $d_k$: Dimension of the key vectors\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zsHvS9msSK8"
      },
      "source": [
        "### Breaking it Down\n",
        "1. $Q$ and $K$ are compared to measure similarity.\n",
        "2. The result is scaled by $\\sqrt{d_k}$ to stabilize gradients.\n",
        "3. We apply $\\text{Softmax}$ to ensure weights sum to 1.\n",
        "4. Multiply weights with $V$ to get the output.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwYptTKOsSK8",
        "outputId": "e3b42b13-5acd-4c32-bbfe-e16a50e81537"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Example Inputs\n",
        "queries = torch.tensor([[1.0, 0.0], [0.0, 1.0]])  # Q\n",
        "keys = torch.tensor([[1.0, 0.0], [0.0, 1.0]])     # K\n",
        "values = torch.tensor([[10.0, 0.0], [0.0, 10.0]]) # V\n",
        "\n",
        "# Compute Attention Scores\n",
        "scores = torch.mm(queries, keys.T) / torch.sqrt(torch.tensor(keys.shape[-1], dtype=torch.float32))\n",
        "weights = F.softmax(scores, dim=-1)\n",
        "output = torch.mm(weights, values)\n",
        "\n",
        "print(\"Attention Weights:\", weights)\n",
        "print(\"Output:\", output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights: tensor([[0.6698, 0.3302],\n",
            "        [0.3302, 0.6698]])\n",
            "Output: tensor([[6.6976, 3.3024],\n",
            "        [3.3024, 6.6976]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ6Lls8bsSK9"
      },
      "source": [
        "---\n",
        "\n",
        "## Building Multi-Head Attention\n",
        "Below, we create a simple multi-head attention layer in PyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcK1nbewsSK9"
      },
      "source": [
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, embed_size, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert embed_size % num_heads == 0, \"Embedding size must be divisible by number of heads.\"\n",
        "\n",
        "        self.head_dim = embed_size // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.query = torch.nn.Linear(embed_size, embed_size)\n",
        "        self.key = torch.nn.Linear(embed_size, embed_size)\n",
        "        self.value = torch.nn.Linear(embed_size, embed_size)\n",
        "        self.fc_out = torch.nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, seq_length, embed_size = x.shape\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "\n",
        "        # Reshape for multi-heads\n",
        "        Q = Q.view(N, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(N, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(N, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "\n",
        "        # Concatenate heads\n",
        "        output = output.transpose(1, 2).contiguous().view(N, seq_length, embed_size)\n",
        "\n",
        "        return self.fc_out(output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10Qs7SnFsSK9",
        "outputId": "4cd34cab-70c0-484b-cb87-366329082f49"
      },
      "source": [
        "import os, sys, platform, datetime, uuid, socket\n",
        "\n",
        "def signoff(name=\"Anonymous\"):\n",
        "    colab_check = \"Yes\" if 'google.colab' in sys.modules else \"No\"\n",
        "    mac_addr = ':'.join(format((uuid.getnode() >> i) & 0xff, '02x') for i in reversed(range(0, 48, 8)))\n",
        "    print(\"+++ Acknowledgement +++\")\n",
        "    print(f\"Executed on: {datetime.datetime.now()}\")\n",
        "    print(f\"In Google Colab: {colab_check}\")\n",
        "    print(f\"System info: {platform.system()} {platform.release()}\")\n",
        "    print(f\"Node name: {platform.node()}\")\n",
        "    print(f\"MAC address: {mac_addr}\")\n",
        "    try:\n",
        "        print(f\"IP address: {socket.gethostbyname(socket.gethostname())}\")\n",
        "    except:\n",
        "        print(\"IP address: Unknown\")\n",
        "    print(f\"Signing off, {name}\")\n",
        "\n",
        "signoff(\"Kushal Chandani\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++ Acknowledgement +++\n",
            "Executed on: 2025-02-02 17:40:45.653004\n",
            "In Google Colab: Yes\n",
            "System info: Linux 6.1.85+\n",
            "Node name: 3cd889875796\n",
            "MAC address: 02:42:ac:1c:00:0c\n",
            "IP address: 172.28.0.12\n",
            "Signing off, Kushal Chandani\n"
          ]
        }
      ]
    }
  ]
}