{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK1lVNp6uRkg"
      },
      "source": [
        "#################################################################\n",
        "#                                                               #\n",
        "#  CS435 Generative AI: Security, Ethics and Governance         #\n",
        "#                                                               #\n",
        "#  Instructor: Dr. Adnan Masood                                 #\n",
        "#  Contact:    adnanmasood@gmail.com                            #\n",
        "#                                                               #\n",
        "#  Notebook is MIT Licensed                                     #\n",
        "#################################################################\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "EK1lVNp6uRkg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9ls_MavuRkh"
      },
      "source": [
        "# **Teaching Tokenization, Vectors, and Embeddings in LLMs**\n",
        "\n",
        "\n",
        "---\n",
        "## Introduction\n",
        "This Jupyter Notebook is designed to explain the concepts of **tokenization**, **vectors**, and **embeddings** in large language models (LLMs). We will explore these topics from five levels of detail (from a very simple, intuitive explanation all the way to a PhD-level perspective). We'll also cover the intuition, brief history, relevant mathematical foundations, illustrative examples, code demonstrations, a step-by-step building of the technology from scratch, real-world applications, and a glossary of terms.\n",
        "\n",
        "Please **run each cell** in this notebook sequentially to follow the entire lesson.\n",
        "---"
      ],
      "id": "O9ls_MavuRkh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4uR3Ew4uRki"
      },
      "source": [
        "# **1. Building an Intuitive Understanding**\n",
        "\n",
        "- **Tokenization**: Splitting long text into small pieces (like breaking a sentence into words or smaller parts) so that a computer can understand it more easily.\n",
        "- **Vectors**: Numbers in a list that help us describe and compare words and their meanings.\n",
        "- **Embeddings**: Special lists of numbers that capture the meaning of words or sentences so that a computer can understand them better.\n",
        "\n",
        "- **Tokenization**: Cutting sentences into the smallest chunks (tokens). These can be whole words or even parts of words, depending on the method.\n",
        "- **Vectors**: Think of them like coordinates on a map. If you want to show how a word relates to other words, you place it in a space with coordinates (numbers) to find words that are close in meaning.\n",
        "- **Embeddings**: A way of placing words (or text) in a multi-dimensional space. Words with similar meanings end up near each other in this space, helping computers figure out relationships between words.\n",
        "\n",
        "- **Tokenization**: In Natural Language Processing (NLP), tokenization can be **word-based** (split on whitespace), **subword-based** (e.g., Byte Pair Encoding/WordPiece), or **character-based**. This helps handle unknown or complex words.\n",
        "- **Vectors**: Vectors are sequences of numbers representing some property. In NLP, vectors often represent how frequently words appear, or how they might cluster in meaning.\n",
        "- **Embeddings**: Vector representations learned by neural networks. They reduce high-dimensional text data into smaller, more meaningful representations. Popular methods include **Word2Vec**, **GloVe**, and **Transformer-based embeddings** (e.g., from BERT or GPT).\n",
        "\n",
        "- **Tokenization**: Advanced tokenization strategies use subword tokenizers (like Byte Pair Encoding in GPT or WordPiece in BERT) that allow efficient handling of rare words or new words (\"unseen tokens\"). This avoids issues like large vocabulary sizes.\n",
        "- **Vectors**: Vectors in NLP can be dense (like embeddings) or sparse (like TF-IDF). Dense embeddings capture semantic relationships, while sparse vectors capture raw frequency-based relationships.\n",
        "- **Embeddings**: They are learned by optimizing an objective that places words (or tokens, subwords, sentences) with similar contexts close together in the embedding space. Contextual embeddings (BERT, GPT, etc.) also consider neighboring words, capturing polysemy (words with multiple meanings) more effectively.\n",
        "\n",
        "- **Tokenization**: Strategies are informed by information theory (maximizing token coverage while minimizing the total vocabulary). Byte-level BPE or SentencePiece are typical for large-scale LLMs. The design affects downstream performance on tasks.\n",
        "- **Vectors & Embeddings**:\n",
        "  - Embeddings can be static (Word2Vec) or contextual (BERT, GPT). Contextual embeddings incorporate attention mechanisms that learn dynamic representations based on the entire sentence.\n",
        "  - Mathematical foundation often involves matrix factorization (GloVe) or neural predictive tasks (Word2Vec skip-gram, CBOW). Transformers rely on self-attention layers and multi-head attention.\n",
        "  - Substantial lines of research focus on embedding interpretability, debiasing, and generalization in multi-lingual or multimodal spaces.\n",
        "\n",
        "---"
      ],
      "id": "U4uR3Ew4uRki"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPsReFq2uRki"
      },
      "source": [
        "# **2. Intuition Behind Tokenization, Vectors, and Embeddings**\n",
        "- **Tokenization**: Imagine reading a book one letter at a time (very slow) versus reading a book word by word (faster and more meaningful). Tokenization helps a computer chunk text into manageable pieces that still preserve meaning.\n",
        "- **Vectors**: If you want to measure how similar two words are, you need a way to compare them. By converting them into a list of numbers (a vector), we can compute distances (or similarities) between the vectors.\n",
        "- **Embeddings**: The better the numbers in the vector capture the word’s meaning, the more accurate the computer’s understanding. Embeddings are these carefully learned numbers, making it so that words like “king” and “queen” end up near each other, while “apple” is farther away.\n",
        "\n",
        "---"
      ],
      "id": "MPsReFq2uRki"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKfGynXAuRki"
      },
      "source": [
        "# **3. Brief History & Underlying Technology**\n",
        "- **Early NLP** used simple bag-of-words or TF-IDF vectors (large sparse vectors). These worked but didn’t capture semantic relationships well.\n",
        "- **Word2Vec (2013)** by Mikolov et al. introduced predictive-based embeddings, popularizing the idea that “you shall know a word by the company it keeps.”\n",
        "- **GloVe (2014)** by Stanford researchers used matrix factorization for global word co-occurrences.\n",
        "- **Contextual Embeddings (ELMo, BERT, GPT, Transformers)** overcame the limitation of a single embedding per word by incorporating context using attention.\n",
        "- **Modern LLMs** (e.g., GPT-3, GPT-4, BERT variants) use tokenization with subword approaches plus self-attention to learn highly sophisticated representations.\n",
        "\n",
        "---"
      ],
      "id": "OKfGynXAuRki"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiPpbgpnuRki"
      },
      "source": [
        "# **4. The Math Behind It**\n",
        "### Word2Vec Skip-Gram Example\n",
        "You try to predict the *context* words given a *target* word. If your target word is $w_t$, and the context words are $w_{t-1}, $w_{t+1}$, etc., then you want:\n",
        "$$\n",
        "P(w_{t-1}, w_{t+1}, ... | w_t)\n",
        "$$\n",
        "\n",
        "We parameterize $w_t$ with vectors (embeddings), say $v_{w_t}$ for the input (target) and $u_{w_c}$ for the output (context). We maximize the log-likelihood:\n",
        "$$\n",
        " \\max \\sum_{t} \\sum_{c \\in \\text{context}(t)} \\log\\, P(w_c | w_t)\n",
        "$$\n",
        "\n",
        "And typically:\n",
        "$$\n",
        " P(w_c | w_t) = \\frac{\\exp(u_{w_c}^T v_{w_t})}{\\sum_{w'} \\exp(u_{w'}^T v_{w_t})}\n",
        "$$\n",
        "\n",
        "When training, we adjust $v_{w_t}$ and $u_{w_c}$ so that words that appear together frequently will have more similar vectors.\n",
        "\n",
        "---"
      ],
      "id": "KiPpbgpnuRki"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z-7ldl_uRkj"
      },
      "source": [
        "# **5. Intuitive Example + Example Calculations with Code**\n",
        "We'll do a small demonstration of how to turn words into vectors using a simple approach with Python.\n",
        "\n",
        "### Illustrative Example with Simple Counting\n",
        "- **Words**: [\"cat\", \"dog\", \"banana\"]\n",
        "- We'll create a simple **co-occurrence** matrix: how many times does each word appear near each other?\n",
        "  - Suppose “cat” and “dog” appear together 2 times\n",
        "  - “cat” and “banana” appear together 0 times\n",
        "  - “dog” and “banana” appear together 1 time\n",
        "\n",
        "This is overly simplistic, but let's see how it might look in code.\n"
      ],
      "id": "1Z-7ldl_uRkj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_LsqptYuRkj",
        "outputId": "09841bc1-4ee2-4245-fd8f-040d86042f5e"
      },
      "source": [
        "# Mock example: constructing a tiny co-occurrence matrix\n",
        "import numpy as np\n",
        "\n",
        "words = [\"cat\", \"dog\", \"banana\"]\n",
        "word_to_index = {word: i for i, word in enumerate(words)}\n",
        "\n",
        "# Suppose these are our co-occurrence counts\n",
        "co_occurrences = {\n",
        "    (\"cat\", \"dog\"): 2,\n",
        "    (\"dog\", \"cat\"): 2,\n",
        "    (\"cat\", \"banana\"): 0,\n",
        "    (\"banana\", \"cat\"): 0,\n",
        "    (\"dog\", \"banana\"): 1,\n",
        "    (\"banana\", \"dog\"): 1\n",
        "}\n",
        "\n",
        "matrix = np.zeros((len(words), len(words)))\n",
        "\n",
        "for (w1, w2), count in co_occurrences.items():\n",
        "    i = word_to_index[w1]\n",
        "    j = word_to_index[w2]\n",
        "    matrix[i,j] = count\n",
        "\n",
        "print(\"Co-occurrence Matrix:\")\n",
        "print(matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Co-occurrence Matrix:\n",
            "[[0. 2. 0.]\n",
            " [2. 0. 1.]\n",
            " [0. 1. 0.]]\n"
          ]
        }
      ],
      "id": "c_LsqptYuRkj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aE8YideuRkj"
      },
      "source": [
        "## Example Calculations (Weights, Bias, etc.)\n",
        "- In a neural network, each **word** is transformed into a **weight vector** (its embedding). We might also have biases for certain layers, but typically in embedding layers, we just store the vectors.\n",
        "- In advanced models, the token embedding is part of a bigger network with attention weights and biases.\n",
        "\n",
        "---"
      ],
      "id": "3aE8YideuRkj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B1M_G7tuRkk"
      },
      "source": [
        "# **6. Step-by-Step Example of Creating a Tiny Embedding from Scratch**\n",
        "We'll do a very simplified version of a **Word2Vec-like** process.\n",
        "\n",
        "### Step-by-Step Outline\n",
        "1. **Collect text**: We'll use a tiny sample corpus.\n",
        "2. **Tokenize**: We'll split it into words.\n",
        "3. **Build vocabulary**: Identify all unique tokens.\n",
        "4. **Initialize embeddings**: Randomly assign each token a small vector of numbers.\n",
        "5. **Define context window**: For each word, look at its neighbors.\n",
        "6. **Predict context**: Train the embeddings by predicting which words appear around the target word.\n",
        "7. **Update embeddings**: Use gradient descent to nudge embeddings for correct predictions.\n",
        "\n",
        "We'll do a miniature version below with a made-up dataset.\n"
      ],
      "id": "2B1M_G7tuRkk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKAPYPypuRkk",
        "outputId": "c1db8ad5-71b0-4261-da24-a5b709bc4aef"
      },
      "source": [
        "# Step-by-step tiny example of a Word2Vec-like approach\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# 1. Tiny sample corpus\n",
        "corpus = [\n",
        "    \"i love dogs\",\n",
        "    \"i love cats\",\n",
        "    \"cats and dogs are friends\"\n",
        "]\n",
        "\n",
        "# 2. Tokenize\n",
        "tokens = [sentence.split() for sentence in corpus]\n",
        "all_tokens = [word for sent in tokens for word in sent]\n",
        "\n",
        "# 3. Build vocabulary\n",
        "vocab = list(set(all_tokens))\n",
        "vocab_size = len(vocab)\n",
        "word_to_idx = {w:i for i, w in enumerate(vocab)}\n",
        "idx_to_word = {i:w for w,i in word_to_idx.items()}\n",
        "\n",
        "# 4. Initialize embeddings\n",
        "embedding_dim = 5  # let's pick 5 for illustration\n",
        "embeddings = np.random.rand(vocab_size, embedding_dim)\n",
        "\n",
        "# 5. Define context window\n",
        "window_size = 2\n",
        "\n",
        "def get_context_pairs(tokens, window_size):\n",
        "    pairs = []\n",
        "    for sent in tokens:\n",
        "        for i, word in enumerate(sent):\n",
        "            target_idx = word_to_idx[word]\n",
        "            start = max(i - window_size, 0)\n",
        "            end = min(i + window_size + 1, len(sent))\n",
        "            for j in range(start, end):\n",
        "                if j != i:\n",
        "                    context_word = sent[j]\n",
        "                    context_idx = word_to_idx[context_word]\n",
        "                    pairs.append((target_idx, context_idx))\n",
        "    return pairs\n",
        "\n",
        "pairs = get_context_pairs(tokens, window_size)\n",
        "\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"Index mapping:\", word_to_idx)\n",
        "print(\"Initial Embeddings Shape:\", embeddings.shape)\n",
        "print(\"Context Pairs (target_idx, context_idx):\")\n",
        "print(pairs[:10], \"...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['cats', 'and', 'i', 'dogs', 'friends', 'love', 'are']\n",
            "Index mapping: {'cats': 0, 'and': 1, 'i': 2, 'dogs': 3, 'friends': 4, 'love': 5, 'are': 6}\n",
            "Initial Embeddings Shape: (7, 5)\n",
            "Context Pairs (target_idx, context_idx):\n",
            "[(2, 5), (2, 3), (5, 2), (5, 3), (3, 2), (3, 5), (2, 5), (2, 0), (5, 2), (5, 0)] ...\n"
          ]
        }
      ],
      "id": "aKAPYPypuRkk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Soxg-gVLuRkk"
      },
      "source": [
        "### 6.1. (Optional) Training Loop\n",
        "A full training loop would do something like:\n",
        "1. For each `(target_idx, context_idx)` pair:\n",
        "2. Compute similarity (e.g., dot product) between `embeddings[target_idx]` and `embeddings[context_idx]`.\n",
        "3. Compare with the desired outcome (we want it to be high if it's a correct pair).\n",
        "4. Backpropagate and update weights.\n",
        "\n",
        "We'll do a toy training iteration with a simplistic approach.\n"
      ],
      "id": "Soxg-gVLuRkk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOdNK2mguRkk",
        "outputId": "a70cf2e1-9d28-4c25-d042-50d05e63c377"
      },
      "source": [
        "import math\n",
        "\n",
        "# We'll define a simple training approach with a dot product + naive gradient\n",
        "learning_rate = 0.01\n",
        "def train_one_epoch(pairs, embeddings, learning_rate=0.01):\n",
        "    for target_idx, context_idx in pairs:\n",
        "        # Dot product\n",
        "        v_target = embeddings[target_idx]\n",
        "        v_context = embeddings[context_idx]\n",
        "\n",
        "        # For simplicity, let's say we want dot product to be close to 1 for real pairs\n",
        "        # error = 1 - (v_target dot v_context)\n",
        "        dot = np.dot(v_target, v_context)\n",
        "        error = 1.0 - dot\n",
        "\n",
        "        # gradient wrt v_target = -error * v_context\n",
        "        grad_target = -error * v_context\n",
        "        grad_context = -error * v_target\n",
        "\n",
        "        # update embeddings\n",
        "        embeddings[target_idx] -= learning_rate * grad_target\n",
        "        embeddings[context_idx] -= learning_rate * grad_context\n",
        "\n",
        "for epoch in range(5):\n",
        "    train_one_epoch(pairs, embeddings, learning_rate)\n",
        "\n",
        "print(\"Embeddings after 5 epochs of naive training:\")\n",
        "print(embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings after 5 epochs of naive training:\n",
            "[[0.09412249 0.57419862 0.5868322  0.29035551 0.29004239]\n",
            " [0.34328585 0.85773915 0.69872871 0.62345482 0.22323943]\n",
            " [0.5306548  0.42530212 0.25891587 0.56119769 0.32581175]\n",
            " [0.36992779 0.53011784 0.64670113 0.2392005  0.19983644]\n",
            " [0.63483106 0.47215001 0.14489468 0.594855   0.07025092]\n",
            " [0.3782707  0.17048342 0.19626395 0.1295491  0.5476497 ]\n",
            " [0.25296625 0.57929295 0.81112216 0.7864826  0.76030657]]\n"
          ]
        }
      ],
      "id": "bOdNK2mguRkk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG3hCXPVuRkk"
      },
      "source": [
        "This is a very crude and simplistic version, but it illustrates how embeddings get updated toward a representation where words co-occurring often end up having a higher dot product.\n",
        "\n",
        "---"
      ],
      "id": "oG3hCXPVuRkk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AEG7LRcuRkk"
      },
      "source": [
        "# **7. What Illustrative Problem Does This Solve?**\n",
        "### Example\n",
        "**Text similarity**: If we can embed sentences, we can measure how similar two sentences are by comparing their vectors. For instance, “I love dogs” and “I like puppies” might have very close embeddings, indicating similar meaning.\n",
        "\n",
        "---\n",
        "# **8. Real-World Problems Solved by This Technology**\n",
        "- **Search/Information Retrieval**: If you type a query, the system can embed the query and find documents with similar embeddings.\n",
        "- **Recommendation Systems**: Embeddings can help match user interests to items.\n",
        "- **Chatbots & Virtual Assistants**: LLMs use embeddings to generate context-aware, coherent responses.\n",
        "\n",
        "---"
      ],
      "id": "_AEG7LRcuRkk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeTqZ-NguRkk"
      },
      "source": [
        "# **9. How to Solve a Real-World Problem Using This Tech**\n",
        "1. **Identify your text data** (e.g., a collection of reviews, articles, or queries).\n",
        "2. **Tokenize** using a robust tokenizer (e.g., from Hugging Face).\n",
        "3. **Get embeddings** for each piece of text (words, sentences, paragraphs) using a pre-trained model or train your own.\n",
        "4. **Perform your task** (e.g., retrieve similar documents, cluster text by topic, power a Q&A chatbot) using vector operations like **cosine similarity**.\n",
        "\n",
        "---"
      ],
      "id": "BeTqZ-NguRkk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDFm9UyVuRkl"
      },
      "source": [
        "# **10. Questions to Illustrate the Use of This Tech**\n",
        "Here are some example questions:\n",
        "1. **How do you measure similarity between two word embeddings?**\n",
        "2. **What happens if a word never appears in training?**\n",
        "3. **How do subword tokenizers solve the out-of-vocabulary problem?**\n",
        "4. **What is the role of attention in creating contextual embeddings?**\n",
        "5. **Can embeddings be updated after deployment?**\n",
        "\n",
        "### Answers and Illustrative Code Examples\n"
      ],
      "id": "zDFm9UyVuRkl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2-PtGwnuRkl"
      },
      "source": [
        "#### Q1: How do you measure similarity between two word embeddings?\n",
        "- Common approach: **Cosine similarity**. We compute:\n",
        "$$\n",
        " \\text{sim}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\|\\,\\|\\mathbf{v}\\|}\n",
        "$$\n"
      ],
      "id": "P2-PtGwnuRkl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAZntDj-uRkl",
        "outputId": "24e38506-bed0-4cb2-c589-248b4a2bd80e"
      },
      "source": [
        "# Example: Cosine similarity\n",
        "def cosine_similarity(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u)*np.linalg.norm(v) + 1e-7)\n",
        "\n",
        "# Let's compare 'i' and 'love' from our toy embeddings (if they exist)\n",
        "if \"i\" in vocab and \"love\" in vocab:\n",
        "    i_embed = embeddings[word_to_idx[\"i\"]]\n",
        "    love_embed = embeddings[word_to_idx[\"love\"]]\n",
        "    sim = cosine_similarity(i_embed, love_embed)\n",
        "    print(\"Cosine similarity between 'i' and 'love':\", sim)\n",
        "else:\n",
        "    print(\"'i' or 'love' not in vocab!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'i' and 'love': 0.8123587807143405\n"
          ]
        }
      ],
      "id": "NAZntDj-uRkl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyr1VRYuuRkl"
      },
      "source": [
        "#### Q2: What happens if a word never appears in training?\n",
        "- If a word (token) never appears, the model can’t learn an embedding directly. **Out-of-vocabulary** words can be handled with **subword tokenizers** or a special \"unknown\" token embedding.\n",
        "\n",
        "#### Q3: How do subword tokenizers solve the out-of-vocabulary problem?\n",
        "- They break a rare word into **subword units**, so even if the entire word is unseen, the pieces likely appear in training.\n",
        "\n",
        "#### Q4: What is the role of attention in creating contextual embeddings?\n",
        "- **Attention** weighs how important each token is to every other token in a sentence, leading to embeddings that reflect the context. Words are interpreted differently depending on surrounding words.\n",
        "\n",
        "#### Q5: Can embeddings be updated after deployment?\n",
        "- Yes. **Continual learning** or further fine-tuning can update embeddings if the model architecture/framework supports it.\n",
        "\n",
        "---"
      ],
      "id": "nyr1VRYuuRkl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aga9pTaTuRkl"
      },
      "source": [
        "# **11. A Sample Exercise**\n",
        "Below is a code skeleton for students to complete. We will generate embeddings using a **HuggingFace** Transformer model (like DistilBERT) to see how it works in practice. You will fill in the missing parts (marked as `TODO`).\n"
      ],
      "id": "Aga9pTaTuRkl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOstMJXnuRkl",
        "outputId": "e16f3aa9-a412-4822-a23a-8c4c9e46e53e"
      },
      "source": [
        "# TODO code sample\n",
        "## Install Transformers if not available\n",
        "# !pip install transformers sentencepiece --quiet  # Uncomment if needed\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# 1. TODO: Choose a pretrained model name, e.g. 'distilbert-base-uncased'\n",
        "model_name = 'xlm-roberta-base'  # using xlm roberta\n",
        "\n",
        "# 2. Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 3. Load model\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "# 4. TODO: Provide some example sentences\n",
        "sentences = [\n",
        "    \"I love machine learning.\",\n",
        "    \"Dogs are awesome pets.\",\n",
        "    \"I love people around me\",\n",
        "    \"I am sure Deepseek gonna go miless\",\n",
        "    \"We gotta watch for qwen by alibaba as well\"\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    # 5. Tokenize sentence\n",
        "    inputs = tokenizer(sentence, return_tensors='pt')\n",
        "\n",
        "    # 6. Forward pass to get hidden states\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # 7. Get the [CLS] token embedding (for DistilBERT, it's the first hidden state)\n",
        "    # or you can average the token embeddings, etc.\n",
        "    last_hidden_state = outputs.last_hidden_state\n",
        "    # Typically, we might take the embedding of the first token or average:\n",
        "    sentence_embedding = torch.mean(last_hidden_state, dim=1)\n",
        "\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(\"Embedding (first 5 values):\", sentence_embedding[0][:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: I love machine learning.\n",
            "Embedding (first 5 values): tensor([ 0.0007,  0.0741,  0.0186, -0.0068,  0.1198])\n",
            "Sentence: Dogs are awesome pets.\n",
            "Embedding (first 5 values): tensor([0.0211, 0.0463, 0.0015, 0.0467, 0.1627])\n",
            "Sentence: I love people around me\n",
            "Embedding (first 5 values): tensor([ 0.0085,  0.0467, -0.0018,  0.0470,  0.1310])\n",
            "Sentence: I am sure Deepseek gonna go miless\n",
            "Embedding (first 5 values): tensor([-0.0174,  0.0237,  0.0121, -0.0037,  0.0929])\n",
            "Sentence: We gotta watch for qwen by alibaba as well\n",
            "Embedding (first 5 values): tensor([-0.0445,  0.0652,  0.0239,  0.0015,  0.1003])\n"
          ]
        }
      ],
      "id": "sOstMJXnuRkl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_CHO_lquRkl"
      },
      "source": [
        "### Hints\n",
        "1. **`model_name`**: Try `'distilbert-base-uncased'` or `'bert-base-uncased'`.\n",
        "2. **Sentences**: Add a few short sentences that you want to test.\n",
        "3. **Embedding**: Observe how the numbers change for different sentences.\n",
        "\n",
        "---"
      ],
      "id": "X_CHO_lquRkl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76qsfai-uRkl"
      },
      "source": [
        "# **12. Glossary**\n",
        "- **Tokenization**: The process of splitting text into tokens (subwords, words, or characters).\n",
        "- **Embedding**: A dense vector that represents a piece of text. Learned during training.\n",
        "- **Vector**: A list/array of numbers. In NLP, used to represent words/documents in numeric form.\n",
        "- **Context Window**: The set of neighboring words around a target word.\n",
        "- **Word2Vec**: Early neural embedding method (Mikolov et al.).\n",
        "- **GloVe**: Global Vectors for Word Representation (Stanford). Uses matrix factorization of co-occurrences.\n",
        "- **Attention**: A mechanism (esp. in Transformers) that helps the model weigh how important each token is with respect to others.\n",
        "- **Transformer**: A neural network architecture using self-attention. BERT, GPT, etc. are based on Transformers.\n",
        "- **LLM (Large Language Model)**: A huge neural model trained on massive text data to understand/generate language.\n",
        "- **Cosine Similarity**: A common measure for how similar two vectors are, based on angle rather than magnitude.\n",
        "- **Out-of-Vocabulary (OOV)**: Words not present in the training set’s vocabulary.\n",
        "- **Subword Tokenizer**: Tokenizes at subword or character level to handle new/rare words effectively.\n",
        "- **[CLS] Token**: A special token used by BERT-like models, often used as a representation for the entire sequence.\n",
        "\n",
        "---\n",
        "## **End of Notebook**\n",
        "Congratulations! You now have an understanding of how tokenization, vectors, and embeddings work in LLMs, from basic concepts to advanced details. Feel free to experiment and modify the code in this notebook.\n",
        "\n",
        "For more reading, check out original papers:\n",
        "- [Word2Vec Paper (Mikolov et al., 2013)](https://arxiv.org/abs/1301.3781)\n",
        "- [GloVe Paper (Pennington et al., 2014)](https://nlp.stanford.edu/pubs/glove.pdf)\n",
        "- [Attention is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "Keep exploring and have fun!"
      ],
      "id": "76qsfai-uRkl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_UcSJ6vuRkl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c320e556-965f-4b0b-c8e3-07fe744539d2"
      },
      "source": [
        "import os, sys, platform, datetime, uuid, socket\n",
        "\n",
        "def signoff(name=\"Anonymous\"):\n",
        "    colab_check = \"Yes\" if 'google.colab' in sys.modules else \"No\"\n",
        "    mac_addr = ':'.join(format((uuid.getnode() >> i) & 0xff, '02x') for i in reversed(range(0, 48, 8)))\n",
        "    print(\"+++ Acknowledgement +++\")\n",
        "    print(f\"Executed on: {datetime.datetime.now()}\")\n",
        "    print(f\"In Google Colab: {colab_check}\")\n",
        "    print(f\"System info: {platform.system()} {platform.release()}\")\n",
        "    print(f\"Node name: {platform.node()}\")\n",
        "    print(f\"MAC address: {mac_addr}\")\n",
        "    try:\n",
        "        print(f\"IP address: {socket.gethostbyname(socket.gethostname())}\")\n",
        "    except:\n",
        "        print(\"IP address: Unknown\")\n",
        "    print(f\"Signing off, {name}\")\n",
        "\n",
        "signoff(\"Kushal Chandani\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++ Acknowledgement +++\n",
            "Executed on: 2025-02-02 17:15:57.217034\n",
            "In Google Colab: Yes\n",
            "System info: Linux 6.1.85+\n",
            "Node name: 37aa1ea50cec\n",
            "MAC address: 02:42:ac:1c:00:0c\n",
            "IP address: 172.28.0.12\n",
            "Signing off, Kushal Chandani\n"
          ]
        }
      ],
      "id": "k_UcSJ6vuRkl"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V91AaK9gqWmJ"
      },
      "id": "V91AaK9gqWmJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}